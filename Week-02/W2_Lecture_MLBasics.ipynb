{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad0edc9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning at the Intersection with Molecular Simulations\n",
    "\n",
    "#### A Brief Survey\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279e22ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the next four weeks, we will cover the following topics.\n",
    "\n",
    "1. Basic concepts\n",
    "2. Neural network architectures\n",
    "3. Generators\n",
    "4. State-of-the-art applications: AlphaFold2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5f544",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is the benefit of ML for simulators?\n",
    "- ML facilitates analysis of complex simulation data and guides sampling.\n",
    "\n",
    "- ML provides 'optimal' force fields.\n",
    "\n",
    "- ML now provides accurate, experiment-like structures of proteins.\n",
    "\n",
    "\"**ML complements physics-based approaches and accelerates the understanding of biology.**\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c1cd8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is the benefit of MD for machine learners?\n",
    "- MD provides data for training, e.g. to learn how to generate dynamics\n",
    "\n",
    "\"**Why do we need to 'understand'? ML will learn and predict everything!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc99354",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the idea of Machine Learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55374896",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![learning](https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-02/learning.png)\n",
    "\n",
    "A model is built (automatically) based on training data:\n",
    "\n",
    "$$\n",
    "  action(dog_i) = f(interactions, treats)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac63e7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![trained](https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-02/trained.png)\n",
    "\n",
    "and then used to predict outcome based on input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af93a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Typical tasks of ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c19d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression \n",
    "\n",
    "**Supervised learning**\n",
    "\n",
    "* *input:* featurized data\n",
    "* *output:* continuous values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c9f8ee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Example:* **Prediction of quantum mechanical energies from molecular configurations**\n",
    "\n",
    "$ <\\Phi_0|e^{-T}He^{T}|\\Phi_0> = E $\n",
    "\n",
    "<img alt=\"iron-sulfur cluster\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/92/OBINIX2.png\" width=\"400\"/>\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfe1e57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data classification \n",
    "\n",
    "**Supervised learning**\n",
    "\n",
    "* *input:* featurized data\n",
    "* *output:* predicted categories\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ccfa4c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Example:* **Prediction of secondary structure from sequence**\n",
    "\n",
    "<img alt=\"secondary structure\" src=\"https://resources.qiagenbioinformatics.com/manuals/clcgenomicsworkbench/current/secondary_structure_output.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c667dda9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dimensionality reduction and feature extraction  \n",
    "\n",
    "**Unsupervised learning**\n",
    "\n",
    "* *input:* high-dimensional data (e.g. MD simulations)\n",
    "* *output:* projection of data onto low-dimensional collective variable space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19a41f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Example:* **Conformational clustering and identifaction of states sampled via MD**\n",
    "\n",
    "<img alt=\"secondary structure\" src=\"https://pubs.acs.org/na101/home/literatum/publisher/achs/journals/content/jpcafh/2021/jpcafh.2021.125.issue-28/acs.jpca.1c02869/20210715/images/medium/jp1c02869_0016.gif\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67524d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generation of high-dimensional data\n",
    "\n",
    "**Supervised learning via advanced deep learning architectures**\n",
    "\n",
    "* *input:* desired features of generated objects\n",
    "* *output:* ensembles of objects based on desired features and consistent with expected properties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34dbbb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Example:* **High-accuracy protein structure generation based on sequence**\n",
    "\n",
    "<img alt=\"AlphaFold2 prediction\" src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-02/alphafold-7m7a.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c6e154",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Models used in ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63fa12",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the most general sense, ML models map input data to output data.\n",
    "\n",
    "$$\n",
    "\\color{red}{Y} = f(\\color{blue}{X}; \\color{green}{W}; \\color{purple}{P}; \\color{grey}{N})\n",
    "$$\n",
    "\n",
    "Model Input:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\color{blue}{\\mbox{X}} &  \\mbox{input data} \\\\\n",
    "\\color{green}{\\mbox{W}}  &  \\mbox{weights to be optimized} \\\\\n",
    "\\color{purple}{\\mbox{P}}  &  \\mbox{model parameters} \\\\\n",
    "\\color{grey}{\\mbox{N}}  &  \\mbox{random noise (optional)} \\\\\n",
    "\\end{matrix} \n",
    "$$\n",
    "\n",
    "The objective of ML is to find an optimal mapping based on training data and other prior knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a30f58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Functional regression\n",
    "\n",
    "Linear regression\n",
    "\n",
    "$ y = ax + b $\n",
    "\n",
    "<img alt=\"linear_regression\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/1024px-Linear_regression.svg.png\" width=\"400\"/>\n",
    "\n",
    "*Uses:* **Prediction of continous value output**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d696d3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Logistic regression\n",
    "\n",
    "Probability of observing certain outcome as a function of input $x$:\n",
    "\n",
    "$ \\log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1*x $\n",
    "\n",
    "<img alt=\"logistic_regression\" src=\"https://upload.wikimedia.org/wikipedia/commons/6/6d/Exam_pass_logistic_curve.jpeg\" width=\"400\"/>\n",
    "\n",
    "*Uses:* **Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be00567",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Support vector machines\n",
    "\n",
    "The goal is to find a hyperplane that maximally separates data into two (or more) classes:\n",
    "\n",
    "$ \\textbf{w}^T \\textbf{x} - b = 0 $\n",
    "\n",
    "with \n",
    "\n",
    "$ \\textbf{w}^T \\textbf{x} - b \\geq 1 $ for $ \\color{blue}{\\mbox{class 1}} $ <BR>\n",
    "$ \\textbf{w}^T \\textbf{x} - b \\leq 1 $ for $ \\color{green}{\\mbox{class 2}} $\n",
    "\n",
    "<img alt=\"support_vector_machine\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/800px-SVM_margin.png\" width=\"300\"/>\n",
    "\n",
    "*Uses:* **Classification**    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130fb9bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision Trees and Random Forests\n",
    "\n",
    "Decision trees generete discrete outcomes from input data based on a series of 'decisions'.\n",
    "\n",
    "Random forest methods combine output from multiple decision trees based on random subsets of input data.  \n",
    "\n",
    "<img alt=\"random_forest\" src=\"https://www.tibco.com/sites/tibco/files/media_entity/2021-05/random-forest-diagram.svg\" width=\"500\"/>\n",
    "\n",
    "*Uses:* **Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62595d07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden Markov Models\n",
    "\n",
    "Markov model describes transition probabilities between (hidden) states $X_i$ and encodes probabilities of possible outcomes $y_i$.\n",
    "\n",
    "<img alt=\"hidden_markov_model\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/HiddenMarkovModel.svg/1920px-HiddenMarkovModel.svg.png\" width=\"400\"/>\n",
    "\n",
    "*Uses:* **Analysis, data encoding, classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9828a165",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural Networks\n",
    "\n",
    "Connect input to output via matrix operations and activation functions. Multiple network layers can be combined.\n",
    "\n",
    "<img alt=\"neural-network\" src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-02/neural-network.png\" width=\"500\"/>\n",
    "\n",
    "*Uses:* **Regression, classification, analysis, encoding, generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db3832",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural Network Architectures\n",
    "\n",
    "Neural networks often involve many layers (deep learning).\n",
    "\n",
    "<img alt=\"mlp\" src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-02/mlp.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa2202",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Activation functions\n",
    "\n",
    "Activation functions are essential for modeling non-linear relationships and for gaining the full benefit of neural networks.\n",
    "\n",
    "<img alt=\"activation_functions\" src=\"https://hsf-training.github.io/hsf-training-ml-webpage/plots/act_functions.png\" width=700/>\n",
    "\n",
    "[Glossary of activation functions](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4514b0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998559a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define problem suitable for ML\n",
    "\n",
    "* What will be the input?\n",
    "* What will be the output?\n",
    "* What are the performance expectations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4d42c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Input data features\n",
    "\n",
    "A good choice of input data features is essential for the success of machine learning. \n",
    "\n",
    "* general molecular properties (composition, mass, charge)\n",
    "* atomic coordinates (subject to rotational variance)\n",
    "* internal coordinates (distances, angles)\n",
    "* dynamic and ensemble information\n",
    "* sequences and multiple sequence alignments\n",
    "* classification based on previous knowledge\n",
    "* experimental data (*e.g.* density maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df44c63a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Target  output data\n",
    "\n",
    "The target output data should reflect the problem at hand and the availability of high-quality training data. \n",
    "\n",
    "* Continuous values (*e.g.* energies, forces, molecular properties)\n",
    "* Classification (*e.g.* secondary structure, topology, function)\n",
    "* Latent space projection\n",
    "* Embedding (encoding of data for further use)\n",
    "* Distributions (*e.g.* distances, angles)\n",
    "* Interactions (*e.g.* intra-/intermolecular contacts, ligands, ions)\n",
    "* Structures (or parts of it)\n",
    "* Dynamic ensembles (or aspects of dynamics)\n",
    "* Quality assessement (*e.g.* model accuracy or experimental uncertainties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf33846",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model design\n",
    "\n",
    "The main consideration is the balance between model complexity, accuracy, and ease of training.\n",
    "\n",
    "* **Neural networks** are flexible and powerful but may be difficult to train\n",
    "* **Deep networks** may be more accurate and transferable but require more data\n",
    "* Deep networks are more difficulty to train than shallow models\n",
    "* Optimal model architecture should **match input/output data shape** (&rightarrow; next week)\n",
    "* Computer hardware may limit model choices\n",
    "\n",
    "Start from established models! (&rightarrow; next week)\n",
    "\n",
    "Model choices and **hyper-parameters** should be optimized as part of model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eae33c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model training\n",
    "\n",
    "The goal of training a specific model is to find optimal weights.\n",
    "\n",
    "This is the key step of machine learning that requires most effort and computer resources.\n",
    "\n",
    "<img alt=\"gpu_card\" src=\"https://assets.nvidia.partners/images/png/3080-ti-back-300x198.png\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618da02f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How do we know which weights are optimal?\n",
    "\n",
    "We define a loss function based on training data, e.g. MSE (mean-squared error) that should be minimized:\n",
    "\n",
    "$$\n",
    "J_{MSE} = \\frac{1}{N} \\sum_{k=1}^{N}(y_k-\\hat{y}_k)^2\n",
    "$$\n",
    "\n",
    "$y_k(\\textbf{x}_{k},\\textbf{w},\\textbf{p})$ is the model-predicted output given weights $\\textbf{w}$ and model parameters $\\textbf{p}$ for training data item $k$.\n",
    "\n",
    "$\\hat{y}_k$ is the expected output from the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d5be1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How do we find optimal weights?\n",
    "\n",
    "We typically use a **gradient descent minimizer** (SGD: Stochastic Gradient Descent; **Adam**):\n",
    "  \n",
    "<img alt=\"gradient_descent\" src=\"http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png\" width=600/>\n",
    "\n",
    "The minimizer updates weights iteratively according to: $w_{new} = w_{old} - \\lambda \\frac{\\partial J(w)}{\\partial w} $\n",
    "\n",
    "$\\lambda$ is the **learning rate** and a key parameter that may be varied during optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cace0f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How do we obtain gradients?\n",
    "\n",
    "Gradients are usually obtained via **back-propagation**, *i.e.* application of the chain rule.\n",
    "\n",
    "<img alt=\"mlp\" src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-02/mlp.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb6c98d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### When do we stop with training?\n",
    "\n",
    "When the loss function stops decreasing. \n",
    "\n",
    "It is better to monitor model performance on a **validation set** and stop when loss function on validation set starts to increase (indicates overfitting). \n",
    "\n",
    "<img alt=\"training_validation\" src=\"https://i.stack.imgur.com/rB6J5.png\" width=500/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f1691",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How do we optimize training performance?\n",
    "\n",
    "* Use **GPUs** or **TPUs** with lots of memory.\n",
    "* **Batch processing** to manage large training data sets.\n",
    "* Adaptive learning rate (start slow, faster steps once optimization converges)\n",
    "* Use pre-trained models and apply **transfer learning**\n",
    "* Explore different model architectures and different activation functions\n",
    "* **Regularize input data** (training is easiest with values in 0-1 range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa412324",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "Data is critical for machine learning. The data should provide reliable mappings between input features and target output data from which a model can be learned.\n",
    "\n",
    "To develop rigorous models, data should be divided into three sets:\n",
    "\n",
    "* **training data** (70-90%): these data are used directly for training the model\n",
    "* **validation data** (10-30%): these data are used for testing model performance during training to determine when to stop and to optimize hyper-parameters\n",
    "* **test data**: these (separate) data are used for evaluating the accuracy of the final models and may consist of benchmarks that allow comparison with other methods or reference data from physical theory or experiments\n",
    "\n",
    "Splitting the initial data into training and validation sets should be repeated randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5620b74",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data biases and incorporation of additional knowledge\n",
    "\n",
    "ML models perform best on domains covered extensively input data and transfer poorly to other scenarios where no or little training data was available. \n",
    "\n",
    "To prevent bias, training data should uniformly cover the broadest possible range of scenarios.\n",
    "\n",
    "**Data augmentation** may provide additional data for areas not well-covered initially, *e.g.* for trivial cases.\n",
    "\n",
    "Training data may be supplemented by **constraints based on other knowledge** that can be applied during training, *e.g.* physical constraints that disallow atom overlap or negative output values for properties that can only assume positive values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d7026",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using ML models\n",
    "\n",
    "Once we have a trained model, the model is easy to apply. All we need is the model architecture and the trained weights.\n",
    "\n",
    "Forward evaluation is usually very fast, but can take time (and require GPU resources) for very large models. More significant time may be needed for **generating the required input features** (*e.g.* multiple sequence alignments as input to AlphaFold2).\n",
    "\n",
    "The usefulness of a given ML model greatly depends on the training data. If the training data is limited, the model, **transferability** (and broader use) is probably also limited. \n",
    "\n",
    "Validation of a given ML model is critical for understanding its expected accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d87ce1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpreting ML models\n",
    "\n",
    "ML models are designed to make highly accurate predictions. That may be sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d647aa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### What if we want to understand things?\n",
    "\n",
    "Direct inspection of optimized weights is usually not productive. Most models are too complex.\n",
    "\n",
    "More insights can be gained from **ablation studies** that remove input features one-by-one to analyze how the model performance after training changes.\n",
    "\n",
    "It may also be possible to gain insights into what information is contained in the training data by challenging the trained model with unexpected input data.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
