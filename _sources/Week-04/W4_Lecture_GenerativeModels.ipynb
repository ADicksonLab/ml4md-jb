{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74933b6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Week 04 Lecture: Generating things with ML \n",
    "\n",
    "#### Generative Models and Molecular Conformational Ensambles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c8bfd5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview for today\n",
    "\n",
    "Today we will cover the following topics:\n",
    "\n",
    "1. What are Generative Models?\n",
    "2. How do people (currently) apply Generative Models in Molecular Sciences?\n",
    "  * Particular attention on: generation of molecular conformations.\n",
    "3. What kind of Generative Models are there?\n",
    "  * Normalizing Flows\n",
    "  * Variational Autoencoders (VAEs)\n",
    "  * Generative Adversarial Networks (GANs)\n",
    "4. What are the limits of Generative Models when generating molecular conformations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd86e0b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1 - What are Generative Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bafc11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What are Generative Models used for?\n",
    "* We have a series of data samples ***x*** in a training set.\n",
    "* There exists a probability distribution *p<sub>data</sub>* for them.\n",
    "* We want a model that learns a distribution *p<sub>model</sub>* to approximate *p<sub>data</sub>*.\n",
    "* Once our model is trained, we want to:\n",
    "  * Sample from *p<sub>model</sub>*.\n",
    "    * Always possible.\n",
    "  * Compute *p<sub>model</sub>(**x**)* for arbitrary samples.\n",
    "    * Not always possible.\n",
    "* Currently, most Generative Models are based on neural networks (NNs). Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52e6ca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/probability_distributions.png\" alt=\"Probability density functions\" title=\"Probability density functions\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b2246",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why NNs? Example of a Generative Model using a NN\n",
    "\n",
    "* Random 1024x1024 pixel samples from StyleGAN2 [[Karras et al., 2019]](https://arxiv.org/abs/1912.04958).\n",
    "    * Number of dimensions: 1024x1024x3=3145728.\n",
    "    * GAN based on a CNN architecture (with a LOT of engineering).\n",
    "    * Training set: 70k images from Flickr.\n",
    "    * Training time: 9 days with 8 Tesla V100 GPUs.\n",
    "* Try it at: [https://thispersondoesnotexist.com](https://thispersondoesnotexist.com).\n",
    "![alt text for screen readers](https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/stylegan2.png \"Examples from the StyleGAN2 article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d78c1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why do we need Machine Learning for Generative Modeling?\n",
    "* The machine learning toolkit allows us to build Generative Models with:\n",
    "    * Statistically-efficient learning:\n",
    "        * Neural networks \"understand\" data better than histograms.\n",
    "        * But exactly how? **Local** interpolation and extrapolation abilities (even experts do not know exactly).\n",
    "    * Computationally-efficient (or at least reasonable) training:\n",
    "        * Time (processing) and space (memory).\n",
    "    * Sampling quality and sampling speed.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8a3d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Biophysical Sciences and complex systems\n",
    "\n",
    "* In Biophysical Sciences, we are interested in complex, highly dimensional systems.\n",
    "* Example: [bacterial ribosome](https://pdb101.rcsb.org/motm/121).\n",
    "<img src=\"https://cdn.rcsb.org/pdb101/motm/121/2wdk_2wdl_front.jpg\" alt=\"Bacterial ribosome\" title=\"Bacterial ribosome\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b8f4c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2 - How do people (currently) apply Generative Models in Molecular Sciences? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb88e175",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Molecular Sciences Applications\n",
    "* Numerous applications.\n",
    "    * Cutting-edge field. So **MUCH** remains to be done!\n",
    "* Some examples:\n",
    "    * Generate new molecular formulas (e.g.: drugs).\n",
    "        * In case you are interested: we will just discuss quickly and will give some references.\n",
    "    * Model the distributions of molecular conformations.\n",
    "        * We will talk about this a lot in the rest of the lecture!\n",
    "        * The field is still in its infancy.\n",
    "        * Lot of room for improvement and exploring new methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7214617",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generate new molecular formulas\n",
    "* Basic principle.\n",
    "    * Find some way to encode molecular formulas in ***x***.\n",
    "    * Collect a dataset of molecules of interest.\n",
    "    * Train a Generative Model (may also use [Reinforcement Learning](https://en.wikipedia.org/wiki/Reinforcement_learning) in the process).\n",
    "    * Sample from it (generate new molecular formulas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef2ee18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generate new molecular formulas: examples\n",
    "* One of the pioneering articles in the field: *MolGAN: An implicit generative model for small molecular graphs* ([link](https://arxiv.org/abs/1805.11973)).\n",
    "* One success story of deep learning and drug development: *Deep learning enables rapid identification of potent DDR1 kinase inhibitors* ([link](https://pubmed.ncbi.nlm.nih.gov/31477924/))\n",
    "* Commentary on the state of the art in this field: *Can Generative-Model-Based Drug Design Become a New Normal in Drug Discovery?* ([link](https://pubmed.ncbi.nlm.nih.gov/34913338/))\n",
    "\n",
    "<img src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/molgan.png\" alt=\"Scheme of molGAN\" title=\"Scheme of molGAN\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e69461",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model the distributions of molecular conformations\n",
    "\n",
    "* Sampling the conformations of a molecular system using \"classical\" approaches:\n",
    "    * Using molecular dynamics (MD) or Monte Carlo-based methods we can sample the distribution of its conformations.\n",
    "    * MD is a very powerful method, but many iterations are need to sample the whole distribution. We need a lot of computing power.\n",
    "\n",
    "<img src=\"https://fold.it/portal/files/images/dill_landscape.png\" alt=\"Energy landscape. From https://pubmed.ncbi.nlm.nih.gov/23180855/\" title=\"Energy landscape. From https://pubmed.ncbi.nlm.nih.gov/23180855/\" width=\"425\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4abe0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Machine learning \"promise\":\n",
    "    * Can we train a generative model that learns the distribution of conformations of molecular systems?\n",
    "    * Can we sample from these distribution faster than \"classical\" methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b877c0b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* What do we need to train a generative model for doing that?\n",
    "    * Training data: conformations from \"classical\" methods (MD). How much? Open question.\n",
    "    * How to best describe ***x***, our conformations? It's an open question:\n",
    "        * xyz coordinates.\n",
    "        * Internal coordinates.\n",
    "    * Train a generative model. Which one? Another open question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d17d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Molecular representation: xyz coordinates\n",
    "* Pros: all the geometric information we need.\n",
    "* Cons: most NNs are not SE(3) invariant.\n",
    "    * Whichever translation or rotatation of our training data, the network should always learn the same thing!\n",
    "    * We must rely on clever tricks to remove translational or rotatational degrees of freedom.\n",
    "* NNs with SE(3) invariances exists, but there is yet no consensus on which one works best, in practice. **Active area of research**.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/39/Alanine-from-xtal-3D-bs-17.png/1024px-Alanine-from-xtal-3D-bs-17.png\" alt=\"Alanine xyz\" title=\"Alanine xyz\" width=\"325\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9dbeb0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Optional: works with some E(3) and SE(3) invariant NNs.\n",
    "    * *E(n) Equivariant Graph Neural Networks.* ([link](https://arxiv.org/abs/2102.09844))\n",
    "    * *E(n) Equivariant Normalizing Flows.* ([link](https://arxiv.org/abs/2105.09016), note: a **generative model**)\n",
    "    * *Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds* ([link](https://arxiv.org/abs/1802.08219))\n",
    "    * *SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks* ([link](https://arxiv.org/abs/2006.10503))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a7c25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Molecular representation: coordinates internal coordinates\n",
    "* E.g.: distance matrices.\n",
    "* Pros: their computation is SE(3) invariant.\n",
    "* Cons: not straightforward to go back to xyz coordinates.\n",
    "\n",
    "<img src=\"https://nanohub.org/app/site/resources/2010/10/09924/5002/1ubq.jpg\" alt=\"Protein distance map\" title=\"Protein distance map\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662b618",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3 - What kind of Generative Models are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518cfc4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Map of Generative Models\n",
    "\n",
    "* Figure copyright and adapted from [NIPS 2016 Tutorial on GANs](https://arxiv.org/abs/1701.00160).\n",
    "\n",
    "<img src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/generative_models_map.png\" alt=\"Generative models map\" title=\"Generative models map\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d7503",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Common characteristics of Generative Models in today's lecture\n",
    "All these models:\n",
    "\n",
    "* Learn from data.\n",
    "* Use neural networks (NNs) as functions approximators.\n",
    "    * Common NN architectures are used. You saw many of them before.\n",
    "    * These NNs are trained with batches using gradient-descent.\n",
    "    * The differences from supervised learning are:\n",
    "        * Input and output of the NNs.\n",
    "        * Loss function that we use.\n",
    "* Generate samples by randomly drawing from a simple prior (e.g.: normal distribution).\n",
    "    * The networks will map noise ***z*** to some object ***x***.\n",
    "    * In other words: it's extremely easy to sample from *p<sub>model</sub>*!\n",
    "* Can be used as **unconditional** or **conditional** models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7369bdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unconditional Generative Models\n",
    "\n",
    "* They learn to approximate *p<sub>data</sub>(**x**)*.\n",
    "* In the figure: random samples from [Image GPT](https://openai.com/blog/image-gpt/).\n",
    "\n",
    "<img src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/unconditional.png\" alt=\"Unconditional model\" title=\"Unconditional model\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf54eaa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional Generative Models (very useful in Molecular Sciences)\n",
    "\n",
    "* They learn to approximate *p<sub>data</sub>(**x** | **y**)*.\n",
    "* What is ***y***? Typical example: a class.\n",
    "* In the image: class-conditioned samples from [[Miyato et al., 2018]](https://arxiv.org/abs/1802.05957).\n",
    "\n",
    "<img src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/conditional.png\" alt=\"Conditional model\" title=\"Conditional model\" width=\"675\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f36cf1a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Actually, ***y*** in *p<sub>data</sub>(**x** | **y**)* can be any variable:\n",
    "    * Parameters of the system (e.g: temperature, pH, etc...).\n",
    "    * Topology (e.g: molecular formula, amino acid sequence, etc...).\n",
    "    * Entire data sample (e.g: conformation at previous time step).\n",
    "    * Anything you can think of...\n",
    "\n",
    "* Conditional models allow us to control what to generate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509adea7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Example from [[Simm and Hernández-Lobato, 2019]](https://arxiv.org/abs/1909.11459):\n",
    "    * They use a Variational Autoencoder to model a distribution *p<sub>data</sub>(**x** | **y**)*.\n",
    "    * ***y***: graph representing a molecule.\n",
    "    * ***x***: molecular conformation.\n",
    "    * If we train with many different molecules (we cover some significant part of the chemical space): allows us to generalize and generate conformations for molecules unseen in training data!\n",
    "    \n",
    "    \n",
    "<img src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/conditional_vae.png\" alt=\"Conditional generation of molecular conformations\" title=\"Conditional generation of molecular conformations\" width=\"625\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467b0bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Normalizing Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056fae43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to learn distributions: maximum likelihood principle\n",
    "\n",
    "* [Maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Relation_to_minimizing_Kullback–Leibler_divergence_and_cross_entropy): important method in statistics.\n",
    "    * Normalizing flows (and other generative models) use it.\n",
    "* Basic principle: our model (with parameters $\\theta$) should assign high probability to the examples in the training set (image from the [NIPS 2016 Tutorial on GANs](https://arxiv.org/abs/1701.00160)).\n",
    "\n",
    "![alt text for screen readers](https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/maximum_likelihood.png \"Examples from the original GAN article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83892bfc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* A maximum likelihood-based loss:\n",
    "\n",
    "    $L = -\\frac{1}{n}$ $\\sum_{i=1}^{n} log(p_{\\theta}(\\boldsymbol{x_i}))$\n",
    "\n",
    "\n",
    "* Here *p<sub>model</sub> = p<sub>$\\theta$</sub>*, we just want highlight that the likelihood depends on $\\theta$.\n",
    "* Train the model: differentiate the loss with respect to $\\theta$ and optimze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac5aa12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optional slide: maximum likelihood and KL divergence\n",
    "* Maximizing the likelihood asymptotically corresponds to minimizing the Kullback-Leibler divergence between *p<sub>data</sub>* and *p<sub>model</sub>* (proof [here](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Relation_to_minimizing_Kullback–Leibler_divergence_and_cross_entropy))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3639e8fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optional slide: a neural network to model ***p<sub>data</sub>*** directly?\n",
    "* Suppose our data ***x*** is continous (e.g.: xyz coordinates or distances).\n",
    "* Any NN is a function approximator *$f(\\boldsymbol x; \\theta)$*.\n",
    "* So why not use NNs to model *p<sub>data</sub>* by training with maximum likelihood? The idea is:\n",
    "\n",
    "    *$p_{model}(\\boldsymbol x) = f(\\boldsymbol x; \\theta)$*\n",
    "    \n",
    "\n",
    "* Will not work.\n",
    "\n",
    "* Probability density functions (*pdf*) are normalized.\n",
    "\n",
    "    *$\\int_{-\\infty}^\\infty p_{model}(x)dx = 1$*\n",
    "\n",
    "    *$p_{model}(x) \\geq 0 \\quad \\forall x$*\n",
    "\n",
    "\n",
    "* NNs are not normalized... will not learn a true *pdf*.\n",
    "* Training outcome: a powerful NN will put extremely high density (tends to infinite) on training datapoints, extremely low on the rest of the dataspace.\n",
    "* We need a partition function... but how to evaluate?\n",
    "* We can still try to model energies with our NN: [Energy Based models](https://en.wikipedia.org/wiki/Energy_based_model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd841e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Normalizing flows principles\n",
    "* Start from a simple prior. E.g. multivariate normal distribution: *p<sub>z</sub> = N(**0**, I)*.\n",
    "    * We can easily sample from it a ***z*** vector.\n",
    "    * We can also easily evaluate its *p<sub>z</sub>(**z**)*.\n",
    "* Our model: a NN function *g* that maps ***z*** to ***x*** (e.g.: a conformation):\n",
    "    * The samples ***x*** = *g(**z**; $\\theta$)* will be distributed according to some (arbitrarly complex) distribution *p<sub>x</sub>*.\n",
    "    * We will use *p<sub>x</sub>* as our *p<sub>model</sub>*. How can we compute *p<sub>x</sub>(**x**)*?\n",
    "    \n",
    "![alt text for screen readers](https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/normalizing_flows.png \"Normalizing flows principle. Figure adapted from https://arxiv.org/abs/1908.09257\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d13c0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We can exactly compute *p<sub>x</sub>(**x**)* if *g*:\n",
    "    * Is an [invertible](https://en.wikipedia.org/wiki/Inverse_function) function.\n",
    "    * We can compute the [determinant of its jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) with respect to ***z***.\n",
    "  \n",
    "* Just use the [change of variable formula](https://en.wikipedia.org/wiki/Integration_by_substitution#Application_in_probability):\n",
    "\n",
    "    $p_x(\\boldsymbol{x}) = p_z(f(\\boldsymbol{x})) |det(Df(\\boldsymbol{x}))|$\n",
    "\n",
    "    \n",
    "* Note, we define:\n",
    "    * *f = g<sup>-1</sup>*.\n",
    "    * *Df(**x**)* is the jacobian of *f(**x**)* with respect to ***x***.\n",
    "* Now we can train using maximum likelihood estimation.\n",
    "    \n",
    "    $ log(p_x(\\boldsymbol{x})) = log(p_z(f(\\boldsymbol{x}))) + log(|det(Df(\\boldsymbol{x}))|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d347a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Normalizing flows in practice\n",
    "\n",
    "* Very elegant method, but poses a restraints on our neural network *g*.\n",
    "    * *g* must be invertible (not difficult to obtain).\n",
    "    * We must be able to compute *det(Df(**x**))* and compute it **efficiently** (difficult to obtain).\n",
    "        * For many of the architectures that you have seen the computation is too expensive.\n",
    "        * How can we use our favourite architectures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b0f7c7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Solutions exists:\n",
    "    * RealNVP from [[Dinh et al., 2016](https://arxiv.org/abs/1605.08803)]. Simple and clever solution (the article is recommended), but we still have to modify our overall network architecture. \n",
    "    * FFJORD from [[Grathwohl et al., 2018](https://arxiv.org/abs/1810.01367)]. No restraints on the architectures, but computationally is slow (must also run a ordinary differential equation solver).\n",
    "    * See this review [[Kobyzev et al., 2019](https://arxiv.org/abs/1908.09257)] for an overview of the challenges in Normalizing Flows.\n",
    "* Active area of research, much room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7c3b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Normalizing flows examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ac128",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Normalizing flows have an important application in molecular conformational ensembles modeling: **Boltzmann generators** [[Noe et al., 2019](https://pubmed.ncbi.nlm.nih.gov/31488660/)].\n",
    "<img src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/boltzmann_generators.png\" alt=\"Boltzmann generators\" title=\"Boltzmann generators\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab9b06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Normalizing flows overview\n",
    "\n",
    "* Pros:\n",
    "    * Good quality samples.\n",
    "    * Sampling speed is good.\n",
    "    * Allow us to compute *p<sub>x</sub>(**x**)*.\n",
    "    * Very popular in conformational ensembles modeling.\n",
    "        * Allows to build Boltzmann generators.\n",
    "    * With new methodolical improvements, could be a very promising method.\n",
    "* Cons:\n",
    "    * Put restraints on neural newtork architectures.\n",
    "        * Not immediately easy to work with.\n",
    "        * Limited model capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90f920",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variational Autoencoders (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc477640",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### VAE overview\n",
    "\n",
    "* Probabilistic models. Conceptually slightly more complex than other models, but very elegant.\n",
    "* There are different ways of interpreting and understading VAEs.\n",
    "    * We will give an high-level and schematic overview of the method.\n",
    "        * **Autoencoder** view of VAEs.\n",
    "    * We will skip many important details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c460a45f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* If you are interested in these topics:\n",
    "    * Encourage you to understand it more deeply.\n",
    "    * Introductory blog post: [link](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73).\n",
    "    * Extremely well-done lecture on VAEs: [video](https://www.youtube.com/watch?v=1CT-kxjYbFU).\n",
    "        * From the Berkley [CS294-158-SP20](https://sites.google.com/view/berkeley-cs294-158-sp20/home) course on generative modeling. Check the other lectures too!\n",
    "    * Introductory review [[Doersch, 2016]](https://arxiv.org/abs/1606.05908).\n",
    "    * Also try the original paper [[Kingma and Welling, 2013]](https://arxiv.org/abs/1312.6114)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1ce37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Autoencoders\n",
    "\n",
    "* A *Encoder* NN (E) takes as input some ***x*** from the training set.\n",
    "* It compresses it to some lower dimensional ***z*** (in a **latent space**).\n",
    "* A *Decoder* NN (D) takes ***z*** and tries to reconstruct ***x***.\n",
    "* How to train? We use a reconstruction loss. Example L2 loss:\n",
    "\n",
    "    $L_{reconstruction} = ||\\boldsymbol{x} - \\boldsymbol{x'}||^2$\n",
    "\n",
    "\n",
    "![alt text for screen readers](https://upload.wikimedia.org/wikipedia/commons/4/4a/VAE_Basic.png \"Autoencoders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a7ee1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We can not easily use vanilla autoencoders as generative models\n",
    "\n",
    "* Generative modeling idea:\n",
    "    * Train an autoencoder.\n",
    "    * Sample from the latent space and reconstruct with the *Decoder*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5acc6e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Problem: how to sample from the latent space?\n",
    "    * Randomly sample from a uniform distribution? Will not work. Most samples will be \"junk\".\n",
    "    * Why: the distribution of points in the latent space is irregular. The *Encoder* \"decided\" it! Example from this [blog post](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf).\n",
    "    \n",
    "<img src=\"https://miro.medium.com/max/1000/1*-i8cp3ry4XS-05OWPAJLPg.png\" alt=\"Unregularized latent space\" title=\"Unregularized latent space\" width=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56773490",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We need some form of regularization: VAEs\n",
    "\n",
    "* VAE solution: we force the points in the latent space to be distributed according to some simple *p<sub>z</sub>* (e.g.: normal). Image from [[Kingma and Welling](https://arxiv.org/abs/1906.02691)].\n",
    "\n",
    "<img src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/vae.png\" alt=\"VAE scheme\" title=\"VAE scheme\" width=\"375\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a016bebf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* VAE solution: modify the loss we use to train our autoencoder.\n",
    "    \n",
    "    $L_{tot} = L_{reconstruction} + L_{regularization}$\n",
    "\n",
    "    \n",
    "* In VAEs, we use as *L<sub>regularization</sub>* the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback–Leibler_divergence) between *q<sub>$\\phi$</sub>(**z** | **x**)* (our probabilistic encoder) and *p<sub>z</sub>(**z**)* (our simple prior).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9683c579",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why are they called variational?\n",
    "\n",
    "* VAEs are probabilistic models! They were derived using a variational approach.\n",
    "* If we treat them as such, we can find out (see the links in previous slides) that their objective is:\n",
    "\n",
    "  <img src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/elbo_1.png\" alt=\"ELBO loss\" title=\"ELBO loss\" width=\"900\" />\n",
    "  where, on the right side, the first term is the regularization objective and the second is the reconstruction objective.\n",
    "\n",
    "* And that:\n",
    "\n",
    "  <img src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/elbo_2.png\" alt=\"Lower bound\" title=\"Lower bound\" width=\"300\" />\n",
    "* Therefore in VAEs we attempt to optimize a variational lower bound of the likelihood of training data points.\n",
    "* For this reason, their objective is also called ELBO (Evidence lower bound).\n",
    "* Formulas are from [[Kingma and Welling, 2013]](https://arxiv.org/abs/1312.6114)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72304a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples in conformational ensembles modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399b378",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Example from [[Simm and Hernández-Lobato, 2019]](https://arxiv.org/abs/1909.11459):\n",
    "    * We saw it before when discussing about conditional models!\n",
    "    * It uses a VAE.\n",
    "    * Good starting point to start studying conformational ensembles modeling and Generative Models.\n",
    "    \n",
    "    \n",
    "<img src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/conditional_vae.png\" alt=\"Conditional generation of molecular conformations\" title=\"Conditional generation of molecular conformations\" width=\"625\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dbe9cc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Example from [[Rosenbaum et al., 2021](https://arxiv.org/abs/2106.14108)]:\n",
    "    * Proof of concept study in which a VAE is used to model the distribution of 3D conformations of a protein that underlies the micrographs in a Cryo-EM experiment.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/cryoem.png\" alt=\"Cryo-EM and VAEs\" title=\"Cryo-EM and VAEs\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5b790c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### VAEs summary\n",
    "* Seems to be the go-to choice of many machine learning practitioners.\n",
    "* Pros:\n",
    "    * Easy-to-use and flexible.\n",
    "    * Training is (usually) stable and quick.\n",
    "    * Many applications in Molecular Sciences.\n",
    "* Cons:\n",
    "    * A good theoretical understanding is necessary to get the most ouf of them (could also be in the \"pros\" list).\n",
    "    * Not great sample quality (need improvements over the vanilla implementation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a6337",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21743ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### History of GANs\n",
    "* From the original GAN article [[Goodfellow et al., 2014](https://arxiv.org/abs/1406.2661)] (highly recommend reading it).\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/gan_2014.png\" alt=\"Examples from the original GAN article\" title=\"Examples from the original GAN article\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e6179",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* From an article some years later [[Karras et al., 2019](https://arxiv.org/abs/1812.04948)] (the first version of StyleGAN).\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/gan_2019.png\" alt=\"Examples from the StyleGAN article\" title=\"Examples from the StyleGAN article\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22940c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GAN overview\n",
    "\n",
    "* In GANs, we have two networks:\n",
    "    * Generator (G):\n",
    "        * Input: noise ***z***. Output: samples ***x***.\n",
    "    * Discriminator (D):\n",
    "        * Input: samples ***x***. Output: probability of ***x*** being \"real\" (from training set) or \"fake\" (from the generator).\n",
    "* Both are trained in an adversarial game.\n",
    "\n",
    "![alt text for screen readers](https://developers.google.com/machine-learning/gan/images/gan_diagram.svg \"Overview of a GAN\")\n",
    "\n",
    "* Image from [Overview of GAN Structure](https://developers.google.com/machine-learning/gan/gan_structure) from *developers.google.com*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf80140",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GAN objective\n",
    "\n",
    "* Let's take a look at the GAN objective:\n",
    "\n",
    "![alt text for screen readers](https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/minmax_loss.png \"GAN loss from the original article\")\n",
    "\n",
    "* *D(**x**)*: probability (a scalar from 0 to 1) of a sample ***x*** to be real.\n",
    "* *G(**z**)*: a generated sample ***x***.\n",
    "\n",
    "\n",
    "* The *D* network wants to:\n",
    "    * Maximize *D(**x**)*.\n",
    "    * Minimize *D(G(**z**))*.\n",
    "* The *G* network wants to:\n",
    "    * Maximize *D(G(**z**))*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e5f7f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Note: we never know how to compute of *p<sub>model</sub>(**x**)*! We can only sample from it via ***x*** = *G(**z**)*.\n",
    "    * GANs are implicit density models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617b9e4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Under ideal conditions, the global optimum of this objective will result in:\n",
    "    * *p<sub>model</sub> = p<sub>data</sub>*\n",
    "    * For proof, see the [original article](https://arxiv.org/abs/1406.2661) (\"Theoretical Results\" section).\n",
    "* But we never have ideal conditions:\n",
    "    * We train with batches.\n",
    "    * We do not have a perfect optimization algorithm.\n",
    "    * G and D have limited capacities.\n",
    "* So... only good approximations of *p<sub>data</sub>* if we are careful enough (see next slide)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ca3dd4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GAN training is notably instable\n",
    "\n",
    "* A common problem is **mode collapse** (see example below from the [NIPS 2016 Tutorial on GANs](https://arxiv.org/abs/1701.00160)).\n",
    "* How to improve training? People have found several solutions that seem to work well:\n",
    "    * WGAN: changes the GAN loss, try to approximate Wasserstein distance. See [[Arjovsky et al., 2017](https://arxiv.org/abs/1701.07875)] and [[Gulrajani et al., 2017](https://arxiv.org/abs/1704.00028)].\n",
    "    * Spectral normalization: regularizes the discriminator. See [[Miyato et al., 2018](https://arxiv.org/abs/1802.05957)].\n",
    "    * And many others (active area of research).\n",
    "* All modern GANs use one of these modifications.\n",
    "\n",
    "![alt text for screen readers](https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/mode_collapse.png \"Example of mode collapse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a0f50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GAN summary\n",
    "* Pros:\n",
    "    * Usually considered the method with best sample quality.\n",
    "        * [Diffusion Models](https://www.youtube.com/watch?v=W-O7AZNzbzQ) are catching up.\n",
    "    * Once trained, they have very fast sampling capabilities.\n",
    "    * In principle, we can use any NN architecture for G and D.\n",
    "* Cons:\n",
    "    * Unstable training.\n",
    "    * Stabilizing may require a LOT of hyper-parameter tuning.\n",
    "    * Usually, training is computational expensive (must update D and then G each mini-batch).\n",
    "    * So far, not many examples in conformational ensembles modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7da0f00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to choose our Generative Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368f076",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some Guidelines on how to Choose our Model\n",
    "\n",
    "\n",
    "|Name|Loss|Explicit Likelihood    |Training stability|Sampling speed|Sample quality|\n",
    "|----|----|-----------------------|------------------|--------------|--------------|\n",
    "|VAE|ELBO|Can be estimated|Good|Fast|Average|\n",
    "|Normalizing Flows|Maximum Likelihood|Yes|Good|Average|Good|\n",
    "|GANs|Adversarial Loss (or others)|No|Bad|Very Fast|Very Good|\n",
    "\n",
    "Note: please take *cum grano salis*! Part of the table is based on clichés. Things will vary with time (e.g.: methodological improvements) and on details of your project and implementation. This table is partly inspired by a table here [[Bond-Taylor et al., 2021](https://arxiv.org/abs/2103.04922)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82614323",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* First narrow down our selection. What do we care about?\n",
    "    * Fast sampling?\n",
    "    * High-quality samples?\n",
    "    * Exact evaluation of *p<sub>model</sub>(**x**)*?\n",
    "    * Amount of fine-tuning (i.e.: time you want to invest in fixing the model)?\n",
    "* Start from what other people have been doing.\n",
    "    * Method development (inventing entirely new things) in machine learning is not easy.\n",
    "        * Trial and error process: often requires a lot of computational resources.\n",
    "* Empirically evaluate different choices.\n",
    "    * In machine learning, that is what people do.\n",
    "* When it works, start introducing some modifications of ours!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a32f1d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4 - What are the limits of Generative Models when generating molecular conformations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc39da58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modeling conformational ensembles: what can we learn?\n",
    "\n",
    "* We have a true data generating distribution *µ*.\n",
    "    * In MD studies this can often be a Boltzmann distribution:\n",
    "        * *p(**x**) = e<sup>-E(x)/(kT)</sup> / Z<sub>x</sub>*\n",
    "* Then we have our training set with ***x***<sup>(1)</sup>, ***x***<sup>(2)</sup>, ..., ***x***<sup>(n)</sup> = {***x***<sup>(i)</sup>}<sup>n</sup><sub>i=1</sub>\n",
    "* Rembember, Generative Models learn from *p<sub>data</sub>*, not from *µ*.\n",
    "    * They do not have access to *µ*.\n",
    "* What if we did we not sample throughly enough *µ* when collecting the samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def96726",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Suppose: our system has *N<sub>states</sub>* metastable states.\n",
    "* In our MD simulation we sample only in one.\n",
    "* We train a generative model on that data only.\n",
    "* Will our model be able to generate samples (with the right probabilities) from other states...?\n",
    "* Most likely, no!\n",
    "* [There is no free lunch](https://en.wikipedia.org/wiki/No_free_lunch_theorem), only local interpolation and extrapolation. \n",
    "\n",
    "![alt text for screen readers](https://github.com/ADicksonLab/ml4md-jb/raw/main/Week-04/energy_landscapes.png \"Energy landscapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42900842",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to recover *µ* and generate entirely new conformations then...?\n",
    "* Open (and difficult) challenge.\n",
    "* Already accurately modeling *p<sub>data</sub>* is not easy.\n",
    "    * Start simple and then build-up.\n",
    "* Solutions?\n",
    "  * Very interesting (incomplete) solution to the problem ([Boltzmann Generators](https://pubmed.ncbi.nlm.nih.gov/31488660/)).\n",
    "  * \"Vanilla\" generative models may be only part of the solution.\n",
    "  * New ideas are necessary!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
