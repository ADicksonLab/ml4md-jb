{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Lab: Machine Learning Force Fields (ML-FF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Name: YOUR NAME HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "### Dataset needed\n",
    "\n",
    "* Before we begin, please start the download the dataset needed for training: [QM dataset needed](https://figshare.com/articles/dataset/ANI-1_data_set_20M_DFT_energies_for_non-equilibrium_small_molecules/5287732). This is a 4.48 GB `tar` file.\n",
    "* Next, please decompress the `tar` file. This can be done on WinOS with [7zip](https://www.7-zip.org), or Linux/Mac with the command `$ tar -xf ANI1_release.tar.gz`\n",
    "\n",
    "### Packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell will install any of the missing packages we'll use in the notebook\n",
    "import sys\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install torchani\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install tensorboard\n",
    "!{sys.executable} -m pip install h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-FF: ANI-1 exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Your Own Neural Network Potential\n",
    "\n",
    "This example shows how to use TorchANI to train a neural network potential\n",
    "with the setup identical to NeuroChem. We will use the same configuration as\n",
    "specified in [`inputtrain.ipt`](https://github.com/aiqm/torchani/blob/master/torchani/resources/ani-1x_8x/inputtrain.ipt). \n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>TorchANI provide tools to run NeuroChem training config file `inputtrain.ipt`.\n",
    "    See: `neurochem-training`.</p></div>\n",
    "\n",
    "<div class=\"alert alert-danger\"><h4>Warning</h4><p>The training setup used in this file is configured to reproduce the original research at \"Less is more: Sampling chemical space with active learning\" (https://aip.scitation.org/doi/10.1063/1.5023802) as much as possible. \n",
    "    That research was done on a different platform called NeuroChem which has many default options and technical details different from PyTorch. Some decisions made here (such as, using NeuroChem's initialization instead of PyTorch's default initialization) is not because it gives better result, but solely based on reproducing the original research. This file should not be interpreted as a suggestions to the readers on how they should setup their models.</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, let's first import the modules and setup devices we will use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchani\n",
    "import os\n",
    "import math\n",
    "import torch.utils.tensorboard\n",
    "import tqdm\n",
    "\n",
    "# helper function to convert energy unit from Hartree to kcal/mol\n",
    "from torchani.units import hartree2kcalmol\n",
    "\n",
    "# device to run the training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's setup constants and construct an Atomic Environment Vector (AEV) computer. \n",
    "* These numbers can be found in [`rHCNO-5.2R_16-3.5A_a4-8.params`](https://github.com/aiqm/torchani/blob/master/torchani/resources/ani-1x_8x/rHCNO-5.2R_16-3.5A_a4-8.params). \n",
    "* The atomic self energies given in [`sae_linfit.dat`](https://github.com/aiqm/torchani/blob/master/torchani/resources/ani-1x_8x/sae_linfit.dat) are computed from ANI-1x\n",
    "dataset. These constants can be calculated for any given dataset if ``None``\n",
    "is provided as an argument to the object of :class:`EnergyShifter` class.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Besides defining these hyperparameters programmatically,\n",
    "  :mod:`torchani.neurochem` provide tools to read them from file.</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These parameters have suffix `r` for the radial and `a` for angular functions (See equations 3 and 4 of [`smith2017-1`](https://doi.org/10.1039/C6SC05720A))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rcr = 5.2000e+00\n",
    "Rca = 3.5000e+00\n",
    "EtaR = torch.tensor([1.6000000e+01], device=device)\n",
    "ShfR = torch.tensor([9.0000000e-01, 1.1687500e+00, 1.4375000e+00, 1.7062500e+00, 1.9750000e+00, 2.2437500e+00, 2.5125000e+00, 2.7812500e+00, 3.0500000e+00, 3.3187500e+00, 3.5875000e+00, 3.8562500e+00, 4.1250000e+00, 4.3937500e+00, 4.6625000e+00, 4.9312500e+00], device=device)\n",
    "Zeta = torch.tensor([3.2000000e+01], device=device)\n",
    "ShfZ = torch.tensor([1.9634954e-01, 5.8904862e-01, 9.8174770e-01, 1.3744468e+00, 1.7671459e+00, 2.1598449e+00, 2.5525440e+00, 2.9452431e+00], device=device)\n",
    "EtaA = torch.tensor([8.0000000e+00], device=device)\n",
    "ShfA = torch.tensor([9.0000000e-01, 1.5500000e+00, 2.2000000e+00, 2.8500000e+00], device=device)\n",
    "species_order = ['H', 'C', 'N', 'O']\n",
    "num_species = len(species_order)\n",
    "aev_computer = torchani.AEVComputer(Rcr, Rca, EtaR, ShfR, EtaA, Zeta, ShfA, ShfZ, num_species)\n",
    "energy_shifter = torchani.utils.EnergyShifter(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember the modified Behler-Parrinello functions from last lecture:**\n",
    "\n",
    "![ANI G-function figure](https://github.com/ADicksonLab/ml4md-jb/blob/main/Week-10/ANI.png?raw=true)\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "f_{\\mathrm{c}} = \\left\\{\n",
    "\\begin{array}{lc}\n",
    "0.5\\left[ \\cos\\left( \\frac{\\pi R_{ij}}{R_{\\mathrm{c}}} \\right) +1 \\right] & \\text{ if } R_{ij} \\leq R_{\\mathrm{c}} \\\\\n",
    "0 & \\text{ otherwise}\n",
    "\\end{array}\n",
    "%\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "The correspondence is as follows:\n",
    "\n",
    "*  `Rcr` : The $R_C$ value in the $f_C$ term for the radial functions (Å)\n",
    "*  `Rca` : The $R_C$ value in the $f_C$ term for the angular functions (Å)\n",
    "*  `EtaR` : A set of $\\eta$ values to use for the radial functions (Å$^{-2}$)\n",
    "*  `ShfR` : A set of $R_s^{(k)}$ values (\"shifts\") to use for the radial functions (Å)\n",
    "*  `Zeta` : A set of $\\zeta$ values to use in the angular functions\n",
    "*  `ShfZ` : A set of $\\theta_s^{(q)}$ values to use in the angular functions (radians)\n",
    "*  `EtaA` : A set of $\\eta$ values to use for the angular functions (Å$^{-2}$)\n",
    "*  `ShfA` : A set of $R_s^{(p)}$ values (\"shifts\") to use for the angular functions (Å)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up your datasets\n",
    "\n",
    "* Now let's setup datasets. These paths assumes the user run this script under the ``examples`` directory of TorchANI's repository. If you download this script, you should manually set the path of these files in your system before this script can run successfully. \n",
    "\n",
    "* Also note that we need to subtracting energies by the self energies of all atoms for each molecule. This makes the range of energies in a reasonable range. The second argument defines how to convert species as a list of string to tensor, that is, for all supported chemical symbols, which is correspond to ``0``, which correspond to ``1``, etc.\n",
    "\n",
    "* See this [Dataset description](https://github.com/isayev/ANI1_dataset#description) for a explanation and details of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WARNING \n",
    "* Please fix the variable `pathprefix` for your system. It must be the path to the folder containing the ANI `h5` files (the ones from the 4.48 GB `tar` file linked at the beginning). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional#\n",
    "pathprefix = \"./ANI-1_release/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional#\n",
    "# Check if the files are in the folder (only check for the first one)\n",
    "import os.path\n",
    "if not os.path.isfile(os.path.join(pathprefix,\"ani_gdb_s01.h5\")):\n",
    "    print(\"Can't find the ANI h5 files in folder \"+pathprefix)\n",
    "    for i in range(5):\n",
    "        print(\"^\")\n",
    "else:\n",
    "    print(\"ANI h5 files located!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    path = os.path.dirname(os.path.realpath(__file__))\n",
    "except NameError:\n",
    "    path = os.getcwd()\n",
    "#dspath = os.path.join(path, '../dataset/ani1-up_to_gdb4/ani_gdb_s01.h5') # <- Original line:\n",
    "dspath = os.path.join(path, pathprefix+'/ani_gdb_s01.h5')\n",
    "batch_size = 2560\n",
    "\n",
    "# Additional#: Here the dataset is split into training and validation set\n",
    "training, validation = torchani.data.load(dspath).subtract_self_energies(energy_shifter, species_order).species_to_indices(species_order).shuffle().split(0.8, None)\n",
    "training = training.collate(batch_size).cache()\n",
    "validation = validation.collate(batch_size).cache()\n",
    "print('Self atomic energies: ', energy_shifter.self_energies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When iterating the dataset, we will get a dict of name->property mapping\n",
    "\n",
    "---\n",
    "**Now let's define atomic neural networks.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up your Architecture\n",
    "\n",
    "**Additional#** Here the different neural networks (one for each atom) are initialized.\n",
    "* The even-index elements of the NNs are the hidden layers. For example, `torch.nn.Linear(160, 128)` means that the previous layer has 160 neurons and the current layer has 128 neurons.\n",
    "* The odd-index elements are the activation functions. The function used is the 'Continuously differentiable Exponential Linear Units' (CELU), see https://pytorch.org/docs/stable/generated/torch.nn.CELU.html#torch.nn.CELU\n",
    "    * Its argument is the alpha value of the CELU formulation (not related to the learning rate)\n",
    "\n",
    "`aev_dim` stores the length (384) of the atomic environment vectors (AEV). See section 2.2 of `smith2017-1`. (Note: Said paper, in page 9, says that the AEVs have length 768. The AEVs were later shortened to 368 without loss of accuracy)\n",
    "\n",
    "All 4 neural networks are loaded on `nn` for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aev_dim = aev_computer.aev_length\n",
    "\n",
    "H_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 160),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(160, 128),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(128, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "C_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 144),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(144, 112),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(112, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "N_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 128),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(128, 112),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(112, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "O_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 128),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(128, 112),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(112, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "nn = torchani.ANIModel([H_network, C_network, N_network, O_network])\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the weights and biases.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Pytorch default initialization for the weights and biases in linear layers\n",
    "  is Kaiming uniform. See: `TORCH.NN.MODULES.LINEAR`_\n",
    "  We initialize the weights similarly but from the normal distribution.\n",
    "  The biases were initialized to zero.</p></div>\n",
    "\n",
    "  https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional#** The Kaiming distribution improves the training of a DNN that uses a ReLU-type (unbounded) activation function rather than sigmoid-type (within the interval (-1,1) or (0,1) ). For a complete account on the topic, see https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79 .\n",
    "\n",
    "This other read is not as didactic but derives the Kaiming initialization https://medium.com/@shoray.goel/kaiming-he-initialization-a8d9ed0b5899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, a=1.0)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "nn.apply(init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now create a pipeline of AEV Computer --> Neural Networks.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchani.nn.Sequential(aev_computer, nn).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's setup the optimizers. NeuroChem uses Adam with decoupled weight decay\n",
    "to updates the weights and Stochastic Gradient Descent (SGD) to update the biases.\n",
    "Moreover, we need to specify different [weight decay rate]( https://arxiv.org/abs/1711.05101) for different layers.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The weight decay in `inputtrain.ipt`_ is named \"l2\", but it is actually not\n",
    "  L2 regularization. The confusion between L2 and weight decay is a common\n",
    "  mistake in deep learning.  See: `Decoupled Weight Decay Regularization`_\n",
    "  Also note that the weight decay only applies to weight in the training\n",
    "  of ANI models, not bias.</p></div>\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Optimization Functions\n",
    "\n",
    "* Here the optimization is set up. \n",
    "* Only the even-index elements are selected since the odd elements are the activation functions.\n",
    "\n",
    "#### The Adam(s) algorithm.\n",
    "The weights are optimized by AdamW, a variant of the Adam optimization method. \n",
    "* A good introduction to the [Adam Optimizer](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) \n",
    "* The original paper [Adam paper](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "* A more general overview of [gradient descent methods](https://arxiv.org/pdf/1609.04747.pdf), which includes SGD and the Adam variants. \n",
    "\n",
    "A few points about Adam:\n",
    "* it can be seen as an extension of SGD.\n",
    "* the method is called Adam (not A.D.A.M. or such). The name is derived from ADAptive Moment estimation.\n",
    "* easy to implement and understand.\n",
    "* appropriate for problems with large amounts of data, parameters, and noisy/sparse gradients.\n",
    "* SGD uses one learning rate for all parameters; Adam uses one for each parameter.\n",
    "* It's a refinement of the momentum optimization method.\n",
    "* The momentum method emulates a ball moving in a ravine: the ball's future position depends on the ravine's gradient AND on the ball's momentum.\n",
    "* The momentum method updates its parameters by\n",
    "\\begin{equation}\n",
    "\\Delta \\theta_i(t) = \\alpha \\cdot \\Delta \\theta_i(t-1) - \\epsilon \\frac{\\partial J}{\\partial \\theta_i}(t)\n",
    "\\end{equation}\n",
    "    * $\\Delta \\theta_i$ is the change in the parameter $\\theta_i$ for the epoch $t$.\n",
    "    * $\\alpha$ is the momentum hyperparameter, usually $\\in [0.5,0.9]$.\n",
    "    * $\\Delta \\theta_i(t-1)$ is the previous change on $\\theta_i$.\n",
    "    * $\\epsilon$ is the learning rate.\n",
    "    * $\\partial J/\\partial \\theta_i$ is the derivative of the cost function wrt $\\theta_i$ for the current epoch.\n",
    "    * It can be seen in the equation above that the Momentum method updates the model's parameters using the *rolling average* of the gradient across the previous iterations, instead of relying solely in the current gradient.\n",
    "* The Adam method takes these refinements one step further, as it not only stores the *rolling average of the gradient*, denoted above as $\\alpha \\cdot \\Delta \\theta_i(t-1)$, but also the *rolling average of the squared gradient*. It uses both to tweak each parameter's learning rate separately. For a deeper explanation of the method, see section 4.6 of the [gradient descent review](https://arxiv.org/pdf/1609.04747.pdf) or alternatively the [original Adam paper](https://arxiv.org/pdf/1412.6980.pdf).\n",
    "* Finally, the AdamW method is a variant of Adam that includes a decoupled weight decay. According to the authors,\n",
    "\n",
    "> we propose a simple modification to recover the original formulation of\n",
    "weight decay regularization by decoupling the weight decay from the optimization\n",
    "steps taken w.r.t. the loss function.\n",
    "\n",
    "> We provide empirical evidence that our proposed modification...(ii) substantially improves Adam’s generalization performance, allowing it to compete with\n",
    "SGD with momentum on image classification datasets (on which it was previously\n",
    "typically outperformed by the latter).\n",
    "\n",
    "For a deeper explanation, see the [authors' paper](https://arxiv.org/pdf/1711.05101.pdf).\n",
    "\n",
    "Note that the biases below are optimized by SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdamW = torch.optim.AdamW([\n",
    "    # H networks\n",
    "    {'params': [H_network[0].weight]},\n",
    "    {'params': [H_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [H_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [H_network[6].weight]},\n",
    "    # C networks\n",
    "    {'params': [C_network[0].weight]},\n",
    "    {'params': [C_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [C_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [C_network[6].weight]},\n",
    "    # N networks\n",
    "    {'params': [N_network[0].weight]},\n",
    "    {'params': [N_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [N_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [N_network[6].weight]},\n",
    "    # O networks\n",
    "    {'params': [O_network[0].weight]},\n",
    "    {'params': [O_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [O_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [O_network[6].weight]},\n",
    "])\n",
    "\n",
    "SGD = torch.optim.SGD([\n",
    "    # H networks\n",
    "    {'params': [H_network[0].bias]},\n",
    "    {'params': [H_network[2].bias]},\n",
    "    {'params': [H_network[4].bias]},\n",
    "    {'params': [H_network[6].bias]},\n",
    "    # C networks\n",
    "    {'params': [C_network[0].bias]},\n",
    "    {'params': [C_network[2].bias]},\n",
    "    {'params': [C_network[4].bias]},\n",
    "    {'params': [C_network[6].bias]},\n",
    "    # N networks\n",
    "    {'params': [N_network[0].bias]},\n",
    "    {'params': [N_network[2].bias]},\n",
    "    {'params': [N_network[4].bias]},\n",
    "    {'params': [N_network[6].bias]},\n",
    "    # O networks\n",
    "    {'params': [O_network[0].bias]},\n",
    "    {'params': [O_network[2].bias]},\n",
    "    {'params': [O_network[4].bias]},\n",
    "    {'params': [O_network[6].bias]},\n",
    "], lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a learning rate scheduler to do learning rate decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdamW_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(AdamW, factor=0.5, patience=100, threshold=0)\n",
    "SGD_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(SGD, factor=0.5, patience=100, threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model by minimizing the MSE loss, until validation RMSE no longer\n",
    "improves during a certain number of steps, decay the learning rate and repeat\n",
    "the same process, stop until the learning rate is smaller than a threshold.\n",
    "\n",
    "We first read the checkpoint files to restart training. We use `latest.pt`\n",
    "to store current training state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_checkpoint = 'latest.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resume training from previously saved checkpoints:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(latest_checkpoint):\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    nn.load_state_dict(checkpoint['nn'])\n",
    "    AdamW.load_state_dict(checkpoint['AdamW'])\n",
    "    SGD.load_state_dict(checkpoint['SGD'])\n",
    "    AdamW_scheduler.load_state_dict(checkpoint['AdamW_scheduler'])\n",
    "    SGD_scheduler.load_state_dict(checkpoint['SGD_scheduler'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we need to validate on validation set and if validation error\n",
    "is better than the best, then save the new best model to a checkpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    # run validation\n",
    "    mse_sum = torch.nn.MSELoss(reduction='sum')\n",
    "    total_mse = 0.0\n",
    "    count = 0\n",
    "    for properties in validation:\n",
    "        species = properties['species'].to(device)\n",
    "        coordinates = properties['coordinates'].to(device).float()\n",
    "        true_energies = properties['energies'].to(device).float()\n",
    "        _, predicted_energies = model((species, coordinates))\n",
    "        total_mse += mse_sum(predicted_energies, true_energies).item()\n",
    "        count += predicted_energies.shape[0]\n",
    "    return hartree2kcalmol(math.sqrt(total_mse / count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use TensorBoard to visualize our training process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = torch.utils.tensorboard.SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "Finally, we come to the training loop.\n",
    "\n",
    "In this tutorial, we are setting the maximum epoch to a very small number,\n",
    "only to make this demo terminate fast. For serious training, this should be\n",
    "set to a much larger value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional# Run the code in this cell if you want to restart the training from zero\n",
    "import os\n",
    "try:\n",
    "    os.remove(\"best.pt\")\n",
    "    os.remove(\"latest.pt\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "print(\"training starting from epoch\", AdamW_scheduler.last_epoch + 1)\n",
    "max_epochs = 10\n",
    "early_stopping_learning_rate = 1.0E-5\n",
    "best_model_checkpoint = 'best.pt'\n",
    "\n",
    "# Additional# This is the big loop for optimization\n",
    "for _ in range(AdamW_scheduler.last_epoch + 1, max_epochs):\n",
    "    rmse = validate()\n",
    "    print('RMSE:', rmse, 'at epoch', AdamW_scheduler.last_epoch + 1)\n",
    "\n",
    "    learning_rate = AdamW.param_groups[0]['lr']\n",
    "\n",
    "    if learning_rate < early_stopping_learning_rate:\n",
    "        break\n",
    "\n",
    "    # checkpoint\n",
    "    if AdamW_scheduler.is_better(rmse, AdamW_scheduler.best):\n",
    "        torch.save(nn.state_dict(), best_model_checkpoint)\n",
    "\n",
    "    # Additional# OPTIMIZATION HAPPENS HERE v\n",
    "    AdamW_scheduler.step(rmse)\n",
    "    SGD_scheduler.step(rmse)\n",
    "\n",
    "    tensorboard.add_scalar('validation_rmse', rmse, AdamW_scheduler.last_epoch)\n",
    "    tensorboard.add_scalar('best_validation_rmse', AdamW_scheduler.best, AdamW_scheduler.last_epoch)\n",
    "    tensorboard.add_scalar('learning_rate', learning_rate, AdamW_scheduler.last_epoch)\n",
    "\n",
    "    for i, properties in tqdm.tqdm(\n",
    "        enumerate(training),\n",
    "        total=len(training),\n",
    "        desc=\"epoch {}\".format(AdamW_scheduler.last_epoch)\n",
    "    ):\n",
    "        species = properties['species'].to(device)\n",
    "        coordinates = properties['coordinates'].to(device).float()\n",
    "        true_energies = properties['energies'].to(device).float()\n",
    "        num_atoms = (species >= 0).sum(dim=1, dtype=true_energies.dtype)\n",
    "        _, predicted_energies = model((species, coordinates))\n",
    "\n",
    "        # Additional# Compute the loss function\n",
    "        loss = (mse(predicted_energies, true_energies) / num_atoms.sqrt()).mean()\n",
    "\n",
    "        AdamW.zero_grad()\n",
    "        SGD.zero_grad()\n",
    "        loss.backward()\n",
    "        AdamW.step()\n",
    "        SGD.step()\n",
    "\n",
    "        #print(\"nn\", nn.state_dict())\n",
    "        #print(\"AdamW\", AdamW.state_dict())\n",
    "        \n",
    "        # write current batch loss to TensorBoard\n",
    "        tensorboard.add_scalar('batch_loss', loss, AdamW_scheduler.last_epoch * len(training) + i)\n",
    "        #print(\"loss\",loss) # custom\n",
    "\n",
    "    torch.save({\n",
    "        'nn': nn.state_dict(),\n",
    "        'AdamW': AdamW.state_dict(),\n",
    "        'SGD': SGD.state_dict(),\n",
    "        'AdamW_scheduler': AdamW_scheduler.state_dict(),\n",
    "        'SGD_scheduler': SGD_scheduler.state_dict(),\n",
    "    }, latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Question 1\n",
    "\n",
    "**1.** As reported in the `smith2017-1` paper (page 9, just above section 4), the RMSE for the training set is of just 1.2 kcal/mol. Can you get close to this value using this notebook?\n",
    "\n",
    "\n",
    "**a)** How does the splitting of data between training, validation, and testing might help?\n",
    "\n",
    "**b)** What other hyperparameters can you change and how do they improve the error estimation?\n",
    "\n",
    "**c)** How does the size of the database change the estimation of error. _Hint:_ Switching the h5 file to one with bigger molecules might get you a smaller error, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Question 2 (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `state_dict()` can be used to display all the data contained in `nn`, `AdamW`, `AdamW_scheduler`, `SGD`, and `SGD_scheduler`.\n",
    "\n",
    "In order to display a particular set of data, see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdamW.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdamW_scheduler.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_scheduler.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key can be added to `state_dict()`, thus extracting a particular piece of information of the objects. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_scheduler.state_dict()['best']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_scheduler.state_dict()['eps']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, the weights of any layer of any element can be obtained by this method.\n",
    "\n",
    "These keys have the format `atNumber`.`layNumber`.weight\n",
    "* `atNumber` refers to the atom: `0` is H, `1` C, `2` N, `3` O\n",
    "* `layNumber` refers to the layer: `0` means the first hidden layer, `2` the second, `4` the third, `6` the fourth (since the odd numbers store the activation functions)\n",
    "\n",
    "The weights can be exported to a file using numpy. Its size can be known by the `shape` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.state_dict()['0.0.weight'] # First hidden layer of H (connects input layer and first hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.state_dict()['0.2.weight'] # Second hidden layer of H (connects first hidden layer and second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.state_dict()['0.0.weight'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below export all weights and biases to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"Weights-ANI\"):\n",
    "    os.mkdir(\"Weights-ANI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nElements = 4\n",
    "listSymbols = [ 'H', 'C', 'N', 'O']\n",
    "listIndicesLayers = [ 0, 2, 4, 6]\n",
    "for i in range(nElements):\n",
    "    for j in range(nElements):\n",
    "        weightsSave = nn.state_dict()[str(i)+'.'+str(listIndicesLayers[j])+'.weight']\n",
    "        np.savetxt('Weights-ANI/weight-'+listSymbols[i]+'-layer'+str(j+1)+'.dat',weightsSave)\n",
    "        biasSave = nn.state_dict()[str(i)+'.'+str(listIndicesLayers[j])+'.bias']\n",
    "        np.savetxt('Weights-ANI/bias-'+listSymbols[i]+'-layer'+str(j+1)+'.dat',biasSave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Question 3 (bonus)\n",
    "\n",
    "Make a plot of predicted vs. calculated energies for one of the training sets.  \n",
    "\n",
    "**Hint: look at the code in the training loop, and use `training[i]` instead of `properties`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography:\n",
    "\n",
    "* This document is based on the notebook found in https://aiqm.github.io/torchani-test-docs/examples/nnp_training.html.\n",
    "However, in this exercise, we add some additional explanations and exercises about ANI that will help the student understand the topic better and can be linked to the lecture better.\n",
    "\n",
    "* `smith2017-1` refers to the ANI-1 paper, available at https://doi.org/10.1039/C6SC05720A.\n",
    "\n",
    "* Other references are included throughout the notebook as direct links"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
