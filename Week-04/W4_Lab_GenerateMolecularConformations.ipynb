{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12f07d7d",
   "metadata": {},
   "source": [
    "# Week 04 Lab: Generating Molecular Conformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b88916",
   "metadata": {},
   "source": [
    "## Student Name: YOUR NAME HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f4849",
   "metadata": {},
   "source": [
    "## Generating molecular conformations with machine learning\n",
    "\n",
    "In this lab we will build a generative model (based on neural networks) that learns how to generate molecular conformations.\n",
    "\n",
    "We will work with the MD simulation data for the \"alanine dipeptide\" molecule. This simple molecule is frequently used as a system to develop and benchmark new methods for the analysis and modeling of molecular dynamics data. An example conformation is shown below:\n",
    "\n",
    "<img alt=\"Alanine dipeptide\" src=\"https://raw.githubusercontent.com/ADicksonLab/ml4md-jb/main/Week-04/alanine_dipeptide.png\" width=\"350px\"/>\n",
    "\n",
    "Note: the molecule is an alanine capped with an acetyl group at the N-terminus and N-methylamide at the C-terminus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb6cfdb",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e998f8fe",
   "metadata": {},
   "source": [
    "We will first download a set of conformations for the alanine dipeptide.\n",
    "\n",
    "The data is freely available at the [mdshare repository](https://markovmodel.github.io/mdshare/ALA2/) (from the  [Computational Molecular Biology Group, Freie Universität Berlin](http://www.mi.fu-berlin.de/en/math/groups/comp-mol-bio/index.html)) and is licensed under [CC BY 4.0](http://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "You can download it from your browser here:\n",
    "* MD data: [alanine-dipeptide-3x250ns-heavy-atom-positions.npz](http://ftp.imp.fu-berlin.de/pub/cmb-data/alanine-dipeptide-3x250ns-heavy-atom-positions.npz)\n",
    "* Topology file: [alanine-dipeptide-nowater.pdb](http://ftp.imp.fu-berlin.de/pub/cmb-data/alanine-dipeptide-nowater.pdb)\n",
    "\n",
    "Instead of downloading it via the browswer, you can use `wget` in the cell below. That may be convenient when using Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d133a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -nc http://ftp.imp.fu-berlin.de/pub/cmb-data/alanine-dipeptide-nowater.pdb\n",
    "# !wget -nc http://ftp.imp.fu-berlin.de/pub/cmb-data/alanine-dipeptide-3x250ns-heavy-atom-positions.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8944d",
   "metadata": {},
   "source": [
    "## Initialize our environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493d090f",
   "metadata": {},
   "source": [
    "Let's import all the libraries that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce25d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import mdtraj\n",
    "try:\n",
    "    import nglview as nv\n",
    "    from nglview import NGLWidget\n",
    "    has_nglview = True\n",
    "except ImportError:\n",
    "    print(\"- Could not import nglview.\")\n",
    "    has_nglview = False\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c360459",
   "metadata": {},
   "source": [
    "Decide which device we want to run on pytorch (which we will use to implement neural networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd69fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"- Using device '%s'\" % device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3322a490",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0d341",
   "metadata": {},
   "source": [
    "### Load xyz data\n",
    "\n",
    "We will work with xyz coordinates of heavy atoms stored in the `alanine-dipeptide-3x250ns-heavy-atom-positions.npz` file.\n",
    "\n",
    "This file stores x, y and z coordinates of three MD trajectories as numpy arrays of shape `(T, N*3)`, where `T` is the number of frames (that is, conformations), `N` is the number of atoms in the system and `3` is for the cartesian coordinates of each atom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ebc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filepath = \"alanine-dipeptide-3x250ns-heavy-atom-positions.npz\"\n",
    "\n",
    "# We want to collect our the frames in this variable.\n",
    "all_xyz = []\n",
    "\n",
    "# Load the data.\n",
    "data = np.load(data_filepath)\n",
    "for key in data:\n",
    "    # Each key is used to store an array.\n",
    "    print(\"- Trajectory with key %s has shape: %s\" % (key, repr(data[key].shape)))\n",
    "    all_xyz.append(data[key])\n",
    "# Concatenate the arrays.\n",
    "all_xyz = np.concatenate(all_xyz)\n",
    "print(\"- All frames are concatenated in an array with shape:\", all_xyz.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c44d1d",
   "metadata": {},
   "source": [
    "### Reshape arrays\n",
    "\n",
    "For analysis purposes, we will reshape the arrays that store our MD data.\n",
    "\n",
    "We start with `(T, N*3)` arrays and want to reshape them to `(T, N, 3)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed49fcd",
   "metadata": {},
   "source": [
    "Insert in the cell below the number of atoms that we have in our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac2f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_atoms = None  # Your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c50666",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_frames = all_xyz.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c380592",
   "metadata": {},
   "source": [
    "Now reshape the array with our data. You can use the numpy `reshape` method for arrays.\n",
    "See https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xyz = None  # Your code here.\n",
    "print(\"- Final xyz array shape:\", all_xyz.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e727366",
   "metadata": {},
   "source": [
    "### Select a random portion of the whole MD data\n",
    "\n",
    "We actually have a lot of MD frames for this simple system (750 ns of MD data). We can assume that the conformational space of this molecule was sampled exhaustively in these simulations. \n",
    "\n",
    "For our purposes today, we will need only a small portion of these frames. Training a generative model with all of the data could take too much time.\n",
    "\n",
    "Let's randomly select only a certain random of frames, these are the ones that we will use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_random_frames = 50000  # You can increase or decrease this number to see how\n",
    "                                 # it will affect our generative modeling study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2d90e",
   "metadata": {},
   "source": [
    "First select some random ids. We first the `numpy.random.choice` function (https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) to generate some random ids. Then we select from our original array the frames corresponding to those ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f4eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ids = np.random.choice(all_xyz.shape[0], number_of_random_frames, replace=False)\n",
    "all_xyz = all_xyz[random_ids]\n",
    "\n",
    "print(\"- Now our MD data has shape:\", all_xyz.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b645b",
   "metadata": {},
   "source": [
    "### Use mdtraj for the analysis and modification of our MD data\n",
    "\n",
    "`mdtraj` has a lot of extremely useful functionalities, we will use some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66529c",
   "metadata": {},
   "source": [
    "First of all, we will initialize a `Topology` object for the heavy-atom representation of our molecule. The details in this cell are not important for today's lecture, so feel free to just run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topology_filepath = \"alanine-dipeptide-nowater.pdb\"\n",
    "allatom_topology = mdtraj.load(topology_filepath).topology\n",
    "heavyatom_topology = allatom_topology.subset(allatom_topology.select(\"element != H\"))\n",
    "heavyatom_topology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c1a892",
   "metadata": {},
   "source": [
    "Then we will create a `Trajectory` object for our MD data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_traj = mdtraj.Trajectory(xyz=all_xyz,\n",
    "                           topology=heavyatom_topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36b5849",
   "metadata": {},
   "source": [
    "Let's take a look at the conformations in our MD data by using `nglview` (if installed).\n",
    "\n",
    "Please note that we randomly selected our MD frames, so the frames in the trajectory will not be in \"chronological\" order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc281e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_nglview:\n",
    "    m_view = nv.show_mdtraj(m_traj)\n",
    "    m_view.center()\n",
    "else:\n",
    "    m_view = None\n",
    "m_view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fcce08",
   "metadata": {},
   "source": [
    "The conformations seem to be somewhat centered. That's lucky! Our generative model will generate xyz coordinates. We will use neural networks that are NOT SE(3) invariant. This means that rotating or translating the conformations could drastically alter the way our networks see the conformations.\n",
    "\n",
    "Since our conformations are centered, we will not need to alter their coordinates. In general, you might want to center your data on some reference conformation if you want to train a machine learning model.\n",
    "\n",
    "We will discuss experiments to see what happens to our generative model if the conformations are not centered.\n",
    "\n",
    "The best way to represent 3D conformation in machine learning is still an open problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be2a9ca",
   "metadata": {},
   "source": [
    "### Analyze distance distributions.\n",
    "\n",
    "In order to check if our generative model is doing a good job at approximating the distribution of conformations in our MD data, we will consider interatomic distances. Right now, we only have xyz coordinates for our MD data, so we will need to compute the corresponding distance matrix data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189bbf5",
   "metadata": {},
   "source": [
    "First, let's print the atoms names and their indices. This will be useful when we will start looking at interatomic distance distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0fac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- Atom indices\")\n",
    "for i, atom_i in enumerate(m_traj.topology.atoms):\n",
    "    print(i, atom_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eba7ed",
   "metadata": {},
   "source": [
    "Here we will compute the distance map. We define a function that we will also use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_xyz_to_dmap(xyz):\n",
    "    \"\"\"\n",
    "    Gets as input a xyz array of shape (T, N, 3), where T is the number of frames and\n",
    "    N is the number of atoms.\n",
    "    Computes the distance matrix for each conformation and returns an new array of\n",
    "    shape (T, N, N).\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(np.square(xyz[:,None,:,:] - xyz[:,:,None,:]), axis=3) + 1e-12)\n",
    "\n",
    "all_dmap = convert_xyz_to_dmap(all_xyz)\n",
    "print(\"- Shape of distance matrix trajectory:\", all_dmap.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2dd2a1",
   "metadata": {},
   "source": [
    "Let's plot some random distance matrices from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45ecd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.random.choice(all_dmap.shape[0], 10, replace=False):\n",
    "    print(\"- Frame\", i)\n",
    "    plt.imshow(all_dmap[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edd3b60",
   "metadata": {},
   "source": [
    "Now let's plot the distance distributions of all the inter-atomic distances in our molecule.\n",
    "\n",
    "Note: if our molecule has `N` atoms, we will have a total of `(Nx(N-1))/2` inter-atomic distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a398b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_atoms):\n",
    "    for j in range(n_atoms):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        plt.title(\"Distance between atoms %s and %s\" % (i, j))\n",
    "        plt.hist(all_dmap[:,i,j], histtype=\"step\", bins=150, density=True)\n",
    "        plt.xlabel(\"nm\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d38ecb",
   "metadata": {},
   "source": [
    "How do the distributions look like? Are there some multi-modal distributions? If you find some some multi-modal distributions, why do you think they are in our MD data for this molecule? And also, how come some distribution are mono-modal and some multi-modal? Please take a moment to briefly discuss!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1ea60",
   "metadata": {},
   "source": [
    "### Ramachandran plot\n",
    "\n",
    "To better understand our data, we will also plot it's Ramachandran plot with a function that uses `mdtraj` and `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6b36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ramachandran_plot(m_traj):\n",
    "    all_phi = mdtraj.compute_phi(m_traj)[1][:,0]\n",
    "    all_psi = mdtraj.compute_psi(m_traj)[1][:,0]\n",
    "    plt.figure(figsize=(4, 4), dpi=100)\n",
    "    plt.hist2d(all_phi, all_psi, bins=200, cmap=plt.cm.nipy_spectral_r,\n",
    "               norm=matplotlib.colors.LogNorm())\n",
    "    plt.title(r\"$p_{data}$\")\n",
    "    # plt.title(r\"$\\mu$\")\n",
    "    plt.xlabel(r\"$\\phi$\")\n",
    "    plt.ylabel(r\"$\\psi$\")\n",
    "    plt.xlim([-np.pi, np.pi])\n",
    "    plt.ylim([-np.pi, np.pi])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5e353",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ramachandran_plot(m_traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c23b36e",
   "metadata": {},
   "source": [
    "Do you notice something that can backup your hypotheses on the presence of multi-modal distributions in our MD data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173fa736",
   "metadata": {},
   "source": [
    "## Build and train a Generative Adversarial Network (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62fe2e",
   "metadata": {},
   "source": [
    "Finally, after inspecting our data, we are ready to build a generative model to model it!\n",
    "\n",
    "In this lab, we will use a [Generative Adverarial Network](https://arxiv.org/abs/1701.00160) (GAN). We covered some theoretical details in the [past lecture](https://adicksonlab.github.io/ml4md-jb/Week-04/W4_Lecture_GenerativeModels.html), but in this notebook we will review again the main concepts of the method.\n",
    "\n",
    "We will use the `pytorch` framework to implement our generative model. To review the basic functionalities of PyTorch, please refer to this [tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html).\n",
    "\n",
    "The GAN implementation that we will use is largely based on this [GAN tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) on the Pytorch website.\n",
    "\n",
    "Feel free to check these tutorials if you want to review and better understand the details of today's lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20238fa",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "A PyTorch `DataLoader` object will be necessary to split our training set in random batches and store our data in `Tensor` objects, which are the PyTorch equivalent for NumPy arrays.\n",
    "\n",
    "Note that the size of our batches is an important hyper-parameter of our neural network training, which you might need to fine-tune empirically. See this [blog post](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/) for an introductory discussion if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca29d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  # This batch size value is probably good for today's learning task.\n",
    "                  # Feel free to experiment with other values!\n",
    "dataloader = torch.utils.data.DataLoader(all_xyz,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frames = 0\n",
    "for b in dataloader:\n",
    "    total_frames += b.shape[0]\n",
    "    print(\"- Current batch shape: %s, total frames so far: %s\" % (\n",
    "          repr(b.shape), total_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4735b6e",
   "metadata": {},
   "source": [
    "### Build our Generator network\n",
    "\n",
    "Time to build our Generator network. We will use a simple [multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) architecture, which should be more than enough to model our small molecule. Check also its image from [Week 2 lecture](https://adicksonlab.github.io/ml4md-jb/Week-02/W2_Lecture_MLBasics.html#neural-networks).\n",
    "\n",
    "The generator is a network that takes as input a random noise vector `z` and maps it to some other vector `x` using some complicated function (we use a MLP). We want to train our generator to learn a function that maps `z` to `x` values that represent \"realistic\" 3D conformations.\n",
    "\n",
    "We will use `16` as the dimension of our latent space from where the `z` vectors are sampled. Again, this is some hyper-parameter that would need fine-tuning, but `16` is probably a good value here. Usually, the dimension of `z` is supposed to be lower than that `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_z = 16  # Latent space dimension.\n",
    "hidden_dim_g = 196  # Number of hidden units in our generator MLP.\n",
    "                    # In GANs, the competing generator and discriminator should be \"balanced\"\n",
    "                    # so we will need to use a similar value for the discriminator (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_z, n_atoms, hidden_dim=32,\n",
    "                 activation=\"relu\",\n",
    "                 layer_norm=None):\n",
    "        \"\"\"\n",
    "        Define our neural network layers.\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_atoms = n_atoms\n",
    "        self.input_layer = nn.Linear(n_z, hidden_dim)\n",
    "        self.act_0 = nn.ReLU()\n",
    "        self.hidden_layer_1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.act_1 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(hidden_dim, n_atoms*3)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        This function is called when we run an instance of our\n",
    "        neural network.\n",
    "        \"\"\"\n",
    "        h = self.input_layer(z)\n",
    "        h = self.act_0(h)\n",
    "        h = self.hidden_layer_1(h)\n",
    "        h = self.act_1(h)\n",
    "        x_gen = self.output_layer(h)\n",
    "        # We reshape to a xyz tensor in the mdtraj (T, N, 3) style.\n",
    "        x_gen = x_gen.reshape(-1, self.n_atoms, 3)\n",
    "        return x_gen\n",
    "\n",
    "net_g = Generator(n_z=n_z, n_atoms=n_atoms, hidden_dim=hidden_dim_g)\n",
    "net_g = net_g.to(device)\n",
    "\n",
    "print(net_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad5bc32",
   "metadata": {},
   "source": [
    "Let's check what the generator is doing. Above you should see a scheme of its architecture.\n",
    "\n",
    "Why is `16` the dimension of its input vectors? Why is `30` the dimension of its output vectors? What do these `30` numbers represent?\n",
    "\n",
    "The cell below will allow you to run our (untrained) generator. What is its output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca8f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Let's generate a batch of 32 random noise vectors.\n",
    "    z_check = torch.randn(32, n_z, device=device)\n",
    "    print(\"- z shape:\", z_check.shape)\n",
    "    # Let's run the generator.\n",
    "    x_check = net_g(z_check)\n",
    "    print(\"- x_gen shape:\", x_check.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238f51a",
   "metadata": {},
   "source": [
    "### Build our Discriminator network\n",
    "\n",
    "Finally, let's build our Discriminator network. We will again use a simple [multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) architecture.\n",
    "\n",
    "The discriminator is a network that takes as input a vector `x` and maps it to a scalar value in the `0-1` interval using some complicated function (we use a MLP).\n",
    "\n",
    "In GAN training, we want to train our discriminator in a [binary classification](https://en.wikipedia.org/wiki/Binary_classification) task, where our two classes are \"fake conformation\" (represented by a `0`) and \"real conformantion\" (represented by a `1`).\n",
    "\n",
    "In GAN training, \"fake\" means a `x` comining from the generator, while \"real\" means a `x` coming from the training data.\n",
    "\n",
    "In other words, when we feed to the discriminator a \"fake\" `x`, we would like it to output a number close to `0`, when we feed it a \"real\" `x`, we would like a output close to `1`.\n",
    "\n",
    "**Optional**. Two last notes about the discriminator.\n",
    "\n",
    "* GAN training is typically instable. There are many techniques that attempt to stabilize it. Today we will use [spectral normalization](https://arxiv.org/abs/1802.05957) on the discriminator weights. It is a computationally light method and it is already implemented as a PyTorch functionality. Its effect is to regularize the discriminator by forcing it to learn a \"smoother\" function, something that appears to stabilize GAN training.\n",
    "\n",
    "* To numerically help neural network training, we also use a [Standard Scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to standardize each atom coordinate using its mean and standard deviation values from the training set. A similar (but more complex) approach is followed for example in [[Noe et al., 2019](https://pubmed.ncbi.nlm.nih.gov/31488660/)].\n",
    "\n",
    "If you want to check what happens without these features, feel free to set our `use_spectral_norm` or `use_scaler` variables to `False`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e17e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim_d = 128\n",
    "use_spectral_norm = True\n",
    "use_scaler = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f62d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.m = None\n",
    "        self.s = None\n",
    "        \n",
    "    def fit(self, xyz):\n",
    "        self.m = torch.tensor(xyz.reshape(-1, n_atoms*3).mean(axis=0), device=device).reshape(1, -1)\n",
    "        self.s = torch.tensor(xyz.reshape(-1, n_atoms*3).std(axis=0), device=device).reshape(1, -1)\n",
    "    \n",
    "    def transform(self, xyz):\n",
    "        return (self.m-xyz)/self.s\n",
    "    \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748c89d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_atoms, hidden_dim=32,\n",
    "                 use_scaler=False,\n",
    "                 use_spectral_norm=False):\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "        self.n_atoms = n_atoms\n",
    "        self.use_scaler = use_scaler\n",
    "        self.use_spectral_norm = use_spectral_norm\n",
    "        \n",
    "        self.input_layer = nn.Linear(n_atoms*3, hidden_dim)\n",
    "        if self.use_spectral_norm:\n",
    "            self.input_layer = torch.nn.utils.spectral_norm(self.input_layer)\n",
    "        self.act_0 = nn.ReLU()\n",
    "        self.hidden_layer_1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        if self.use_spectral_norm:\n",
    "            self.hidden_layer_1 = torch.nn.utils.spectral_norm(self.hidden_layer_1)\n",
    "        self.act_1 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "        if self.use_spectral_norm:\n",
    "            self.output_layer = torch.nn.utils.spectral_norm(self.output_layer)\n",
    "        self.act_sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, self.n_atoms*3)\n",
    "        if self.use_scaler:\n",
    "            x = scaler.transform(x)\n",
    "        h = self.input_layer(x)\n",
    "        h = self.act_0(h)\n",
    "        h = self.hidden_layer_1(h)\n",
    "        h = self.act_1(h)\n",
    "        o = self.output_layer(h)\n",
    "        o = self.act_sigmoid(o)\n",
    "        return o\n",
    "\n",
    "net_d = Discriminator(n_atoms=n_atoms, hidden_dim=hidden_dim_d,\n",
    "                      use_spectral_norm=use_spectral_norm,\n",
    "                      use_scaler=use_scaler)\n",
    "net_d = net_d.to(device)\n",
    "\n",
    "print(net_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8008627a",
   "metadata": {},
   "source": [
    "Let's check what the discriminator is doing. Above you should see a scheme of its architecture.\n",
    "\n",
    "Why is `30` the dimension of its input vectors? Why is `1` the dimension of its output?\n",
    "\n",
    "The cell below will allow you to run our (untrained) discriminator. What is its output? Notice how its output is a number between 0 and 1 (an untrained discriminator should output some random number close to `0.5`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # We extract a batch of training data.\n",
    "    x_check = next(dataloader.__iter__())\n",
    "    x_check = x_check.to(device)\n",
    "    print(\"- x_ref shape:\", x_check.shape)\n",
    "    # We give it as input to the discriminator.\n",
    "    out_check = net_d(x_check)\n",
    "    print(\"- discriminator output shape:\", out_check.shape)\n",
    "    print(\"- First 10 elements of the output:\", out_check[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875183f4",
   "metadata": {},
   "source": [
    "## Optimizers and Loss Functions\n",
    "\n",
    "We are almost ready to train our GAN! Let's create our `Adam` optimizers to perform gradient-based optimization of neural network parameters. [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) is a more sophisticated version of the basic [Steepest Descent](https://en.wikipedia.org/wiki/Gradient_descent) algorithm (which you may know from Molecular Mechanics), it is the default choice of almost all machine learning practitioners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e35919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "learning_rate = 0.00025\n",
    "beta1 = 0.25\n",
    "optimizer_d = optim.Adam(net_d.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "optimizer_g = optim.Adam(net_g.parameters(), lr=learning_rate, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5ced7",
   "metadata": {},
   "source": [
    "Here we create our loss function. Today, we will use a slightly modified version of the [original GAN objective]().\n",
    "\n",
    "This modification is called the non-saturated GAN loss and was discussed in the original [GAN article](https://arxiv.org/abs/1406.2661) as a form of improvement over the original minmax objective.\n",
    "\n",
    "Here is its formula (image from [[Che et al., 2020](https://arxiv.org/abs/2003.06060)]):\n",
    "\n",
    "<img alt=\"non-saturated GAN loss\" src=\"https://raw.githubusercontent.com/ADicksonLab/ml4md-jb/main/Week-04/non_saturated_gan_loss.png\" width=\"500px\"/>\n",
    "\n",
    "\n",
    "Where:\n",
    "* *L<sub>D</sub>* is the discriminator loss and *L<sub>G</sub>* is the generator loss.\n",
    "* *D(**x**)* is the discriminator output (a scalar from 0 to 1).\n",
    "* *G(**z**)* is the generator output, a \"fake\" conformation ***x***.\n",
    "* The [Expected_value](https://en.wikipedia.org/wiki/Expected_value) is computed over:\n",
    "    * *p<sub>data</sub>* (something that we approximate by sampling batches of conformation from our training set)\n",
    "    * *p<sub>z</sub>* (something that we approximate by generating random samples by sampling from our simple prior).\n",
    "\n",
    "In our GAN implemenentation, for each mini-batch will first train the discriminator by using *L<sub>D</sub>*, and then the generator using *L<sub>G</sub>*.\n",
    "\n",
    "What is the goal of training with these functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09157948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function, which we will use to compute L_D and L_G.\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2579909",
   "metadata": {},
   "source": [
    "Create batch of latent vectors that we will use to visualize the progression of the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35040773",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_fixed = torch.randn(10000, n_z, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f08367e",
   "metadata": {},
   "source": [
    "## Functions to train the model\n",
    "\n",
    "Now we are ready to train! In the cell below there is function that implement GAN training for `num_epochs`. In machine learning a training epoch is a pass through all the training examples. GAN training can be quite slow sometimes, so you may need to train at least 50 epochs to see some acceptable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94acaebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_kl_approximation(v_true, v_pred, n_bins=50, pseudo_c=0.00001):\n",
    "    # Define bins.\n",
    "    _min = min((v_true.min(), v_pred.min()))\n",
    "    _max = max((v_true.max(), v_pred.max()))\n",
    "    bins = np.linspace(_min, _max, n_bins+1)\n",
    "    # Compute the frequencies in the bins.\n",
    "    ht = (np.histogram(v_true, bins=bins)[0]+pseudo_c)/v_true.shape[0]\n",
    "    hp = (np.histogram(v_pred, bins=bins)[0]+pseudo_c)/v_pred.shape[0]\n",
    "    kl = -np.sum(ht*np.log(hp/ht))\n",
    "    return kl, bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7242566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "\n",
    "    # Lists to keep track of progress.\n",
    "    history = []\n",
    "\n",
    "    print(\"Starting Training Loop...\\n\")\n",
    "\n",
    "    # For each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "    \n",
    "        print(\"\\n# Epoch %s of %s\" % (epoch, num_epochs))\n",
    "        \n",
    "        # We set to training mode our networks.\n",
    "        net_d.train()\n",
    "        net_g.train()\n",
    "\n",
    "        epoch_data = {\"loss_d\": 0, \"loss_g\": 0,\n",
    "                      \"D_x\": 0,\n",
    "                      \"D_G_z1\": 0,\n",
    "                      \"D_G_z2\": 0,\n",
    "                      \"epoch_elements\": 0}\n",
    "\n",
    "        # For each batch in the dataloader\n",
    "        for i, x_real in enumerate(dataloader, 0):\n",
    "\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "\n",
    "            ## Train with all-real batch\n",
    "\n",
    "            # Zero-out the gradients of netD.\n",
    "            net_d.zero_grad()\n",
    "\n",
    "            # Format batch\n",
    "            x_real = x_real.to(device)\n",
    "            b_size = x_real.size(0)\n",
    "\n",
    "            # Store the number of samples processed in this epoch.\n",
    "            epoch_data[\"epoch_elements\"] += b_size\n",
    "\n",
    "            # Create a tensor with shape (b_size,) with only 1 values,\n",
    "            # since 1 is the label of real samples.\n",
    "            label = torch.ones((b_size,), dtype=torch.float, device=device)\n",
    "\n",
    "            # Forward pass real batch through D.\n",
    "            # We use.view(-1) to reshape from (b_size, 1) -> (b_size, )\n",
    "            out_real = net_d(x_real).view(-1)\n",
    "\n",
    "            # Calculate loss on all-real batch.\n",
    "            # We want to maximize log(D(x)).\n",
    "            loss_d_real = criterion(out_real, label)\n",
    "\n",
    "            # Calculate gradients for D in backward pass\n",
    "            loss_d_real.backward()\n",
    "\n",
    "            # Stores data for calculating D_x for this epoch.\n",
    "            epoch_data[\"D_x\"] += out_real.sum().item()\n",
    "\n",
    "\n",
    "            ## Train with all-fake batch\n",
    "            # Generate batch of latent vectors\n",
    "            z = torch.randn(b_size, n_z, device=device)\n",
    "\n",
    "            # Generate fake image batch with G\n",
    "            x_fake = net_g(z)\n",
    "\n",
    "            # Now we fill the label tensor with 0, since that is the label of fakes\n",
    "            # samples.\n",
    "            label.fill_(fake_label)\n",
    "\n",
    "            # Classify all fake batch with D.\n",
    "            # Note: detach() is needed, since we want to take x_fake out of the\n",
    "            # computational graph: we are not interested in calculating the gradient\n",
    "            # with respect to netG parameters now (we must do it later when updating\n",
    "            # netG). This means that we are treating x_fake as some \"constant\" input\n",
    "            # for the netD (just like x_true).\n",
    "            out_fake = net_d(x_fake.detach()).view(-1)\n",
    "\n",
    "            # Calculate D's loss on the all-fake batch.\n",
    "            # We want to minimize log(1 - D(G(z)).\n",
    "            loss_d_fake = criterion(out_fake, label)\n",
    "\n",
    "            # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "            loss_d_fake.backward()\n",
    "\n",
    "            # Compute the accuracy of netD predictions.\n",
    "            epoch_data[\"D_G_z1\"] += out_fake.sum().item()\n",
    "\n",
    "            # Compute error of D as sum over the fake and the real batches\n",
    "            loss_d = loss_d_fake + loss_d_fake\n",
    "\n",
    "            # Store the loss_d values.\n",
    "            epoch_data[\"loss_d\"] += loss_d.item()*b_size\n",
    "\n",
    "            # Update D\n",
    "            optimizer_d.step()\n",
    "\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "\n",
    "            # Zero-out netG gradients.\n",
    "            net_g.zero_grad()\n",
    "\n",
    "            label.fill_(real_label)  # fake labels are real for generator cost\n",
    "\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "            out_fake = net_d(x_fake).view(-1)\n",
    "\n",
    "            # Calculate G's loss based on this output.\n",
    "            # We want to maximize log(D(G(z))).\n",
    "            loss_g = criterion(out_fake, label)\n",
    "\n",
    "            # Store the loss_g values.\n",
    "            epoch_data[\"loss_g\"] += loss_g.item()*b_size\n",
    "\n",
    "            # Calculate gradients for G\n",
    "            loss_g.backward()\n",
    "\n",
    "            # Calculate the accuracy for D after it was updated.\n",
    "            epoch_data[\"D_G_z2\"] = out_fake.sum().item()\n",
    "\n",
    "            # Update G\n",
    "            optimizer_g.step()\n",
    "        \n",
    "        # We set to eval mode our networks.\n",
    "        net_d.eval()\n",
    "        net_g.eval()\n",
    "        \n",
    "        # Generate random conformations to check how training is going.\n",
    "        with torch.no_grad():\n",
    "            x_gen_fixed = net_g(z_fixed).cpu().numpy()\n",
    "            dmap_gen_fixed = convert_xyz_to_dmap(x_gen_fixed)\n",
    "        \n",
    "        # Evaluate all the inter-atomic distance distributions.\n",
    "        kl_list = []\n",
    "        for i in range(n_atoms):\n",
    "            for j in range(n_atoms):\n",
    "                if i >= j:\n",
    "                    continue\n",
    "                dist_ref = all_dmap[:,i, j]\n",
    "                dist_gen = dmap_gen_fixed[:,i, j]\n",
    "                kl_i = score_kl_approximation(dist_ref, dist_gen)[0]\n",
    "                kl_list.append(kl_i)\n",
    "                \n",
    "        # Updates the history (divide each value by the number of elements\n",
    "        # processed in this epoch, so that we have average values for the epoch.\n",
    "        for key in list(epoch_data.keys()):\n",
    "            if key == \"epoch_elements\":\n",
    "                continue\n",
    "            epoch_data[key] = epoch_data[key]/epoch_data[\"epoch_elements\"]\n",
    "        epoch_data[\"average_kld\"] = np.mean(kl_list)\n",
    "        history.append(epoch_data)\n",
    "        \n",
    "        # Output training process stats.\n",
    "        print('[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f'\n",
    "              % (epoch, num_epochs,\n",
    "                 epoch_data[\"loss_d\"], epoch_data[\"loss_g\"],\n",
    "                 epoch_data[\"D_x\"], epoch_data[\"D_G_z1\"]))\n",
    "        print(\"\\tAverage KLD for all inter-atomic distances: %s\" % round(np.mean(kl_list), 4))\n",
    "\n",
    "        # Plot some distance distributions.\n",
    "        if epoch % 5 == 0:\n",
    "            n_hist = 4\n",
    "            fig, ax = plt.subplots(1, n_hist, figsize=(12, 3.0))\n",
    "            h_args = {\"histtype\": \"step\", \"density\": True, \"bins\": 50}\n",
    "            for k in range(n_hist):\n",
    "                atm_i, atm_j = np.random.choice(n_atoms, 2, replace=False)\n",
    "                dist_ref = all_dmap[:,atm_i, atm_j]\n",
    "                dist_gen = dmap_gen_fixed[:,atm_i, atm_j]\n",
    "                kl_i = score_kl_approximation(dist_ref, dist_gen)[0]\n",
    "                ax[k].set_title(\"Distance %s-%s (KL = %s)\" % (atm_i, atm_j, round(kl_i, 3)))\n",
    "                ax[k].hist(dist_ref, label=\"MD\", **h_args)\n",
    "                ax[k].hist(dist_gen, label=\"GEN\", **h_args)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b1f0a6",
   "metadata": {},
   "source": [
    "Here you can run the training. Start with 50 epochs. If you feel that the results are not yet acceptable, run again this cell to train for more epochs (you can also modify the number of epochs).\n",
    "\n",
    "During training, we will evaluate the divergence between the reference (from MD) and generated (from our GAN) distance distributions using an approximation of the [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback–Leibler_divergence). The lower this score is, the more the two distribution are similar (if distributions are equal, KLD would be 0).\n",
    "\n",
    "Check your average KLD! If, after some epochs it is not improving, there is probably no point in training longer.\n",
    "\n",
    "To visually check how well our generator is modeling the distance distributions, we also plot some histograms every 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888dc6bb",
   "metadata": {},
   "source": [
    "## Evaluate our GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa37a22e",
   "metadata": {},
   "source": [
    "When you are done with training it's time to evaluate our GAN and take a look at some of the generated conformations.\n",
    "\n",
    "First let's generate the same number of conformations that we have in our training set. We will also time how much it takes for the generator to sample these conformations.\n",
    "\n",
    "Note that all of these will be independent samples. It would have probably taken much longer to generate via MD the same number of statistically-independent samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78142ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "with torch.no_grad():\n",
    "    t0 = time.time()\n",
    "    n_eval = all_xyz.shape[0]\n",
    "    z_eval = torch.randn(n_eval, n_z, device=device)\n",
    "    x_gen_eval = net_g(z_eval).cpu().numpy()\n",
    "    t1 = time.time()\n",
    "    dmap_gen_eval = convert_xyz_to_dmap(x_gen_eval)\n",
    "print(\"- Generated xyz array shape:\", x_gen_eval.shape)\n",
    "print(\"- It took %s seconds to generate %s independant conformations.\" % (\n",
    "      t1-t0, x_gen_eval.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969c898",
   "metadata": {},
   "source": [
    "First let's plot all the inter-atomic distance histograms. Is the GAN capturing multi-modal distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e3d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_atoms):\n",
    "    for j in range(n_atoms):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        kl_i = score_kl_approximation(all_dmap[:,i,j], dmap_gen_eval[:,i,j])[0]\n",
    "        plt.title(\"Distance between atoms %s and %s (KL =  %s)\" % (\n",
    "                  i, j, round(kl_i, 4)))\n",
    "        plt.hist(all_dmap[:,i,j], histtype=\"step\", bins=150, density=True, label=\"MD\")\n",
    "        plt.hist(dmap_gen_eval[:,i,j], histtype=\"step\", bins=150, density=True, label=\"GEN\")\n",
    "        plt.xlabel(\"nm\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c7356",
   "metadata": {},
   "source": [
    "The let's take a look at the 3D conformations using `mdtraj` and `nglview`. Do they look kind of real to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccedf98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_traj_gen = mdtraj.Trajectory(xyz=x_gen_eval,\n",
    "                               topology=heavyatom_topology)\n",
    "\n",
    "if has_nglview:\n",
    "    m_view_gen = nv.show_mdtraj(m_traj_gen)\n",
    "    m_view_gen.center()\n",
    "else:\n",
    "    m_view_gen = None\n",
    "m_view_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4197a661",
   "metadata": {},
   "source": [
    "Finally, let's plot the Ramachandran plot for our generated ensemble. Does it bear some resemblance to the one observed in MD data? What are the most important differences? Is our GAN capturing all the metastable states in the system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d213456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- Generated Ramachandran plot\")\n",
    "show_ramachandran_plot(m_traj_gen)\n",
    "\n",
    "print(\"\\n- MD Ramachandran plot\")\n",
    "show_ramachandran_plot(m_traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267348a4",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858dbb7f",
   "metadata": {},
   "source": [
    "Congratulations! You have modeled the your first conformational ensemble of a molecule with a generative model!\n",
    "\n",
    "Please take a moment to pause and discuss what are in your opinion the strenghts and weaknesses that you found for this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fe0cac",
   "metadata": {},
   "source": [
    "Unfortunately, machine learning is highly dependant on the training data. You can get a feeling of this be running again this notebook and using only a selected part of the MD data. Instead of using 50000 random conformations, we will use the first 50000 conformations in the MD data: the simulation from which this data comes from, only sampled part of the conformational ensemble of the alanine dipeptide.\n",
    "\n",
    "You can select this data by restarting the notebook and substituting the line `all_xyz = all_xyz[random_ids]` with `all_xyz = all_xyz[:20000]`.\n",
    "\n",
    "When you train the GAN only with this data, is it going to be able to generate conformations for all the conformational ensemble?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
